{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MPI and Scalapy to distribute all of MICA workflow to work on multiple nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep.py component \\\n",
    "Read input file and slice into manageable sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython import parallel\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch an ipython parallel cluster\n",
    "Run this on hpc node to launch a cluster with mpi engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this currently needs to be launched from terminal\n",
    "#We need to launch an ipython parallel cluster\n",
    "#!ipcluster start --engines=MPIEngineSetLauncher --log-level DEBUG --n=4 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a parallel client so that we can use %%px cell magic\n",
    "# With rc and dview, we can interact between mpi ranks and the thread running this notebook\n",
    "#rc = ipp.Client(profile='mvapich',sshserver='cburdysh@noderome209')\n",
    "#rc = ipp.Client(profile='mvapich')\n",
    "rc = ipp.Client()\n",
    "\n",
    "\n",
    "dview = rc[:]\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#load all necessary libraries onto each rank\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "#from mpi4py import MPI\n",
    "import sys\n",
    "import numba\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy as sci\n",
    "import numpy as np\n",
    "import anndata\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "import fast_histogram\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from MICA.lib import utils\n",
    "#from scalapy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from scalapy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to make sure MPI (mpi4py) is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] noderome234[11406]: 0/4\n",
      "[stdout:1] noderome234[11407]: 1/4\n",
      "[stdout:2] noderome234[11408]: 2/4\n",
      "[stdout:3] noderome234[11409]: 3/4\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import os\n",
    "import socket\n",
    "#from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "print(\"{host}[{pid}]: {rank}/{size}\".format(\n",
    "    host=socket.gethostname(),\n",
    "    pid=os.getpid(),\n",
    "    rank=comm.rank,\n",
    "    size=comm.size,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin execution of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import os\n",
    "cwd=os.getcwd()\n",
    "if rank==0:\n",
    "    print(cwd)\n",
    "    \n",
    "data_file_path = cwd+'/test_data/inputs/10x/PBMC/3k/pre-processed/'\n",
    "input_file_name = data_file_path + 'pbmc3k_preprocessed.h5ad'\n",
    "project_name = 'pbmc3k'\n",
    "output_file_name = data_file_path+project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#set slice size (max size of row blocks)\n",
    "slice_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print (input_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Prep_dist() to split file into slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_0.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_1.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_2.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_3.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_4.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stderr:0] \n",
      "/home/cburdysh/.conda/envs/py36/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n",
      "/home/cburdysh/.conda/envs/py36/lib/python3.6/site-packages/anndata/_core/anndata.py:1192: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if is_string_dtype(df[key]) and not is_categorical(df[key])\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#Run prep.py only on one processor to create the slice files\n",
    "g_nrows=0 #global number of rows (cells)\n",
    "ncols=0\n",
    "nslices=0\n",
    "if rank==0: \n",
    "    #g_nrows, ncols, nslices = prep_dist(input_file_name, output_file_name, slice_size)\n",
    "    g_nrows, ncols, nslices = utils.prep_dist(input_file_name, output_file_name, slice_size)\n",
    "    \n",
    "#broadcast resultant variables from root to the other ranks\n",
    "g_nrows = comm.bcast(g_nrows, root=0)\n",
    "ncols = comm.bcast(ncols, root=0)\n",
    "nslices = comm.bcast(nslices, root=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] global nrows, ncols, slices:  2496 10499 5\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print(\"global nrows, ncols, slices: \",g_nrows, ncols, nslices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in anndata preprocessed files (in distributed mode, by node number) and calculate distance metrics between all row pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "block comparsons = 15. jobs per rank = 3\n",
      "\n",
      "rank:  0  comparison between segs: 0  x  0  symmetric= True\n",
      "rank:  0  comparison between segs: 0  x  1  symmetric= False\n",
      "rank:  0  comparison between segs: 0  x  2  symmetric= False\n",
      "rank:  0  comparison between segs: 0  x  3  symmetric= False\n",
      "Elapsed = 31.496451377868652\n",
      "[stdout:1] \n",
      "rank:  1  comparison between segs: 0  x  4  symmetric= False\n",
      "rank:  1  comparison between segs: 1  x  1  symmetric= True\n",
      "rank:  1  comparison between segs: 1  x  2  symmetric= False\n",
      "rank:  1  comparison between segs: 1  x  3  symmetric= False\n",
      "Elapsed = 30.980910062789917\n",
      "[stdout:2] \n",
      "rank:  2  comparison between segs: 1  x  4  symmetric= False\n",
      "rank:  2  comparison between segs: 2  x  2  symmetric= True\n",
      "rank:  2  comparison between segs: 2  x  3  symmetric= False\n",
      "rank:  2  comparison between segs: 2  x  4  symmetric= False\n",
      "Elapsed = 31.283909559249878\n",
      "[stdout:3] \n",
      "rank:  3  comparison between segs: 3  x  3  symmetric= True\n",
      "rank:  3  comparison between segs: 3  x  4  symmetric= False\n",
      "rank:  3  comparison between segs: 4  x  4  symmetric= True\n",
      "Elapsed = 19.579569101333618\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#create a 2d list to hold blocks of similarity matrix\n",
    "#this should be stored in a distributed scalapack matrix\n",
    "b=nslices #row blocks\n",
    "SM = [[None for j in range(b)] for i in range(b)] \n",
    "\n",
    "start = time.time()\n",
    "utils.calc_distance_metric_distributed(data_file_path, project_name, g_nrows, ncols, nslices, SM)\n",
    "end = time.time()\n",
    "print(\"Elapsed = %s\" % (end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px \n",
    "####from scipy.sparse import csr_matrix #may not use csr as it complicates copy to distributed scalapack and is not used in scalapack apparently\n",
    "#import collections\n",
    "#for i in range(b):\n",
    "#    for j in range(i,b):\n",
    "#        if isinstance(SM[i][j], collections.Iterable):\n",
    "#            #print(\"Rank:\",rank, \" SM[\",i,\"][\",j,\"]=\",SM[i][j])\n",
    "#            print(\"SM[\",i,\"][\",j,\"]=\",SM[i][j],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create distributed matrix for scalapack and copy distributed blocks into object\n",
    "### This matrix needs to be dense for use in scalapack functions, so we will copy the symmetric data into both upper and lower triangular sections of the MI matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy lower triangular transpose to upper triangular for diagonal blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "##from scipy.sparse import csr_matrix #may not use csr as it complicates copy to distributed scalapack and is not used in scalapack apparently\n",
    "import collections\n",
    "for i in range(b):\n",
    "    for j in range(i,b):\n",
    "        if isinstance(SM[i][j], collections.Iterable):\n",
    "            if i==j: #copy lower triangular transpose to upper triangular \n",
    "                for ii in range(SM[i][j].shape[0]):\n",
    "                    for jj in range(ii+1,SM[i][j].shape[1]):\n",
    "                        (SM[i][j])[ii,jj]=(SM[i][j])[jj,ii]\n",
    "                #print(\"Rank:\",rank, \" SM[\",i,\"][\",j,\"]=\",SM[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate a global array with all of the MI data from each rank\n",
    "\n",
    "Preferably, we would like each rank to contribute of their block MI matrices to the global matrix,\n",
    "but currently the distributed global matrix has to be constructed from a global (not distributed) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy SM data into global distributed matrix and then write to file?\n",
    "\n",
    "#then we can read that file into the Scalapack block cyclic matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] PR= 2 PC= 2\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#test to distribute matrix from local blocks rather than global array\n",
    "from scalapy import blacs\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from mpi4py import MPI\n",
    "from scalapy import core\n",
    "import scalapy.routines as rt\n",
    "\n",
    "#distribute MI components to ranks as scalapack distributed matrix\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size #total number of ranks\n",
    "\n",
    "global_num_rows =g_nrows\n",
    "global_num_cols =g_nrows\n",
    "local_num_rows =g_nrows/b\n",
    "\n",
    "block_size=64 #default is 32\n",
    "\n",
    "#Define process grid with process rows and process cols\n",
    "#We'll use a 2d process grid to distribute blocks so we want to have num_ranks divisible by 2\n",
    "assert((size % 2)==0)\n",
    "#ideally we would like BR and BC to the square root of the num_ranks to get a square process matrix\n",
    "PR=int(np.sqrt(size))\n",
    "PC=PR\n",
    "\n",
    "#if we can't create a square matrix, get next best dimensions\n",
    "if PR*PR!=size:\n",
    "    PC=size//PR\n",
    "if rank==0:\n",
    "    print(\"PR=\",PR, \"PC=\",PC)\n",
    "\n",
    "#sets default context and block_shape\n",
    "core.initmpi([PR, PC],block_shape=[block_size,block_size])\n",
    "#convert to fortran array indexing to match scalapack functions\n",
    "#create global matrix from array on rank0\n",
    "dMI=core.DistributedMatrix(global_shape=[g_nrows,g_nrows],dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "##get global indices for diagonal\n",
    "#gi, lri, lci = dMI.local_diagonal_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI = %s' % (rank, dMI.global_shape))\n",
    "#print ('rank %d has local_shape of dMI = %s' % (rank, dMI.local_shape))\n",
    "#print ('rank %d has block_shape of dMI = %s' % (rank, dMI.block_shape))\n",
    "#print(dMI.local_array[lri,lci])\n",
    "#print(dMI.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#blocksize=slice_size\n",
    "#testrank=3\n",
    "#testmat=np.zeros(shape=(4,4))\n",
    "#if comm.rank==testrank:\n",
    "#    testmat=SM[3][3]\n",
    "#    #testmat=np.zeros(shape=(500,500))\n",
    "#    s_block_shape=np.shape(testmat)\n",
    "#else:\n",
    "#    testmat=np.zeros(shape=(4,4))\n",
    "#    s_block_shape=np.shape(testmat)\n",
    "\n",
    "#s_block_shape = comm.bcast(s_block_shape, root=testrank)   \n",
    "#copy_from_np(dMI2, testmat, asrow=0, anrow=None, ascol=0, ancol=None, srow=0, scol=0, block_shape=s_block_shape, rank=testrank) #all ranks works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy each SM block submatrix to distributed block cyclic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "copy SM[ 0 0 ] shape:  (500, 500)  to global i,j: 0 0\n",
      "copy SM[ 0 1 ] shape:  (500, 500)  to global i,j: 0 500\n",
      "copy SM[ 0 2 ] shape:  (500, 500)  to global i,j: 0 1000\n",
      "copy SM[ 0 3 ] shape:  (500, 500)  to global i,j: 0 1500\n",
      "[stdout:1] \n",
      "copy SM[ 0 4 ] shape:  (500, 496)  to global i,j: 0 2000\n",
      "copy SM[ 1 1 ] shape:  (500, 500)  to global i,j: 500 500\n",
      "copy SM[ 1 2 ] shape:  (500, 500)  to global i,j: 500 1000\n",
      "copy SM[ 1 3 ] shape:  (500, 500)  to global i,j: 500 1500\n",
      "[stdout:2] \n",
      "copy SM[ 1 4 ] shape:  (500, 496)  to global i,j: 500 2000\n",
      "copy SM[ 2 2 ] shape:  (500, 500)  to global i,j: 1000 1000\n",
      "copy SM[ 2 3 ] shape:  (500, 500)  to global i,j: 1000 1500\n",
      "copy SM[ 2 4 ] shape:  (500, 496)  to global i,j: 1000 2000\n",
      "[stdout:3] \n",
      "copy SM[ 3 3 ] shape:  (500, 500)  to global i,j: 1500 1500\n",
      "copy SM[ 3 4 ] shape:  (500, 496)  to global i,j: 1500 2000\n",
      "copy SM[ 4 4 ] shape:  (496, 496)  to global i,j: 2000 2000\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "blocksize=slice_size\n",
    "n_jobs_per_rank= (int((b * (b + 1)) / 2))/comm.Get_size()\n",
    "import collections\n",
    "for i in range(b):\n",
    "    for j in range(i,b): # j in range [i,b]\n",
    "        idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "        srank = idx//n_jobs_per_rank\n",
    "        lA=np.zeros(shape=(2,2))\n",
    "        s_block_shape=np.shape(lA)\n",
    "        if isinstance(SM[i][j], collections.Iterable):\n",
    "            lA=SM[i][j]\n",
    "            s_block_shape=np.shape(lA)\n",
    "            print(\"copy SM[\",i,j,\"] shape: \",s_block_shape,\" to global i,j:\",i*blocksize,j*blocksize)\n",
    "        #broadcast sending ranks block shape to all\n",
    "        s_block_shape = comm.bcast(s_block_shape, root=srank)   \n",
    "        dMI.np2self(lA, srow=i*blocksize, scol=j*blocksize, block_shape=s_block_shape, rank=srank )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI = %s' % (rank, dMI.global_shape))\n",
    "#print ('rank %d has local_shape of dMI = %s' % (rank, dMI.local_shape))\n",
    "#print ('rank %d has block_shape of dMI = %s' % (rank, dMI.block_shape))\n",
    "#print(dMI.local_array)\n",
    "##print(dMI.local_array[lri,lci])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy transpose of blocks to fill upper triangular distributed matrix (needed for scalapack computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "blocksize=slice_size\n",
    "n_jobs_per_rank= (int((b * (b + 1)) / 2))/comm.Get_size()\n",
    "import collections\n",
    "\n",
    "for i in range(b):\n",
    "    for j in range(i+1,b): # j in range [i,b]\n",
    "        idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "        srank = idx//n_jobs_per_rank\n",
    "        lA=np.zeros(shape=(2,2))\n",
    "        s_block_shape=np.shape(lA)\n",
    "        if isinstance(SM[i][j], collections.Iterable):\n",
    "            lA=np.transpose(SM[i][j])\n",
    "            s_block_shape=np.shape(lA)\n",
    "            #print(\"copy SM[\",j,i,\"] shape: \",s_block_shape)\n",
    "        #broadcast sending ranks block shape to all\n",
    "        s_block_shape = comm.bcast(s_block_shape, root=srank)   \n",
    "        dMI.np2self(lA, srow=j*blocksize, scol=i*blocksize, block_shape=s_block_shape, rank=srank )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to also fill in empty symmetric upper triangular portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though this is a symmetric matrix, for further processing, we need to copy block data to rest of matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI = %s' % (rank, dMI.global_shape))\n",
    "#print ('rank %d has local_shape of dMI = %s' % (rank, dMI.local_shape))\n",
    "#print ('rank %d has block_shape of dMI = %s' % (rank, dMI.block_shape))\n",
    "##print(dMI.local_array[0:20,0:20])\n",
    "#print(dMI.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "###lr=[range(4)]\n",
    "###lc=[range(4)]\n",
    "####Check to see of data was transferred to rank 0\n",
    "#if rank==0:\n",
    "#    for i in range(b):\n",
    "#            for j in range(i,b): # j in range [i,b]\n",
    "#                #print(\"SM[\",i,j,\"] \",np.shape(SM[i][i]))\n",
    "#                print(\"SM[\",i,j,\"] \",SM[i][j])\n",
    "##                print( (SM[i][j])[lr,lc] )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#from inspect import getmembers, isfunction, ismodule\n",
    "#if rank == 0:\n",
    "#    print([o[0] for o in getmembers(scalapy) if ismodule(o[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#from inspect import getmembers, isfunction, ismodule\n",
    "#import scalapy\n",
    "#if rank==0:\n",
    "#    print(getmembers(scalapy.blacs, isfunction))\n",
    "#    #print(getmembers(scalapy.routines, isfunction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#total number of global blocks = total number of block comparisons\n",
    "#Should be greater than number of ranks to improve load balancing\n",
    "# b is number of slices (blocks of rows) original data has been discretized into.\n",
    "#The size of these blocks can be variable\n",
    "#global_number_of_matrix_blocks= int((b * (b + 1)) / 2) \n",
    "\n",
    "#We'll use a 2d process grid to distribute blocks so we want to have num_ranks divisivle by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bcast SM[i][j] from each rank to root rank 0 so that we can load global matrix array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write distributed MI matrix to file\n",
    "### So we can read this in to Scalapack later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#Write MI matrix to file\n",
    "mi_filename = data_file_path+project_name+'_mi_distributed.scalapack'\n",
    "dMI.to_file(mi_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code snippet reads MI matrix from a file and loads it into a distributed Scalapack matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:17]: \u001b[0m<scalapy.core.DistributedMatrix at 0x2aab50f4d4a8>"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2021-06-29T18:54:24.925673",
      "data": {},
      "engine_id": 0,
      "engine_uuid": "f32862bc-3b03cabde92b8c87f06022ef",
      "error": null,
      "execute_input": "#Read MI matrix from file\nmi_filename = data_file_path+project_name+'_mi_distributed.scalapack'\ndMI.from_file(mi_filename, global_shape=[g_nrows,g_nrows], dtype=np.float64, block_shape=[block_size,block_size])\n",
      "execute_result": {
       "data": {
        "text/plain": "<scalapy.core.DistributedMatrix at 0x2aab50f4d4a8>"
       },
       "execution_count": 17,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "49d55353-30cc5a86fad6721a518020f2_65",
      "outputs": [],
      "received": "2021-06-29T18:54:24.927107",
      "started": "2021-06-29T18:54:24.829651",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2021-06-29T18:54:24.827018"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:17]: \u001b[0m<scalapy.core.DistributedMatrix at 0x2aaaf3002470>"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2021-06-29T18:54:24.914595",
      "data": {},
      "engine_id": 1,
      "engine_uuid": "db2b407b-e66a2134b651f89fd0eea0fb",
      "error": null,
      "execute_input": "#Read MI matrix from file\nmi_filename = data_file_path+project_name+'_mi_distributed.scalapack'\ndMI.from_file(mi_filename, global_shape=[g_nrows,g_nrows], dtype=np.float64, block_shape=[block_size,block_size])\n",
      "execute_result": {
       "data": {
        "text/plain": "<scalapy.core.DistributedMatrix at 0x2aaaf3002470>"
       },
       "execution_count": 17,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "49d55353-30cc5a86fad6721a518020f2_66",
      "outputs": [],
      "received": "2021-06-29T18:54:24.917063",
      "started": "2021-06-29T18:54:24.829732",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2021-06-29T18:54:24.827410"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:17]: \u001b[0m<scalapy.core.DistributedMatrix at 0x2aab50fdeef0>"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2021-06-29T18:54:24.914373",
      "data": {},
      "engine_id": 2,
      "engine_uuid": "2531e04a-6aa307511303cb96ceb0048d",
      "error": null,
      "execute_input": "#Read MI matrix from file\nmi_filename = data_file_path+project_name+'_mi_distributed.scalapack'\ndMI.from_file(mi_filename, global_shape=[g_nrows,g_nrows], dtype=np.float64, block_shape=[block_size,block_size])\n",
      "execute_result": {
       "data": {
        "text/plain": "<scalapy.core.DistributedMatrix at 0x2aab50fdeef0>"
       },
       "execution_count": 17,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "49d55353-30cc5a86fad6721a518020f2_67",
      "outputs": [],
      "received": "2021-06-29T18:54:24.915966",
      "started": "2021-06-29T18:54:24.829752",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2021-06-29T18:54:24.827520"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:17]: \u001b[0m<scalapy.core.DistributedMatrix at 0x2aaaee905518>"
      ]
     },
     "metadata": {
      "after": [],
      "completed": "2021-06-29T18:54:24.914478",
      "data": {},
      "engine_id": 3,
      "engine_uuid": "03e9a39f-eb3437b09735427ece4ef0e9",
      "error": null,
      "execute_input": "#Read MI matrix from file\nmi_filename = data_file_path+project_name+'_mi_distributed.scalapack'\ndMI.from_file(mi_filename, global_shape=[g_nrows,g_nrows], dtype=np.float64, block_shape=[block_size,block_size])\n",
      "execute_result": {
       "data": {
        "text/plain": "<scalapy.core.DistributedMatrix at 0x2aaaee905518>"
       },
       "execution_count": 17,
       "metadata": {}
      },
      "follow": [],
      "msg_id": "49d55353-30cc5a86fad6721a518020f2_68",
      "outputs": [],
      "received": "2021-06-29T18:54:24.916670",
      "started": "2021-06-29T18:54:24.829862",
      "status": "ok",
      "stderr": "",
      "stdout": "",
      "submitted": "2021-06-29T18:54:24.827604"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "#Read MI matrix from file\n",
    "mi_filename = data_file_path+project_name+'_mi_distributed.scalapack'\n",
    "dMI.from_file(mi_filename, global_shape=[g_nrows,g_nrows], dtype=np.float64, block_shape=[block_size,block_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI = %s' % (rank, dMI2.global_shape))\n",
    "#print ('rank %d has local_shape of dMI = %s' % (rank, dMI2.local_shape))\n",
    "#print ('rank %d has block_shape of dMI = %s' % (rank, dMI2.block_shape))\n",
    "#print(dMI2.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to create a normalization matrix\n",
    "### We start with an empty matrix but add the the diagonal as the first column\n",
    "### Then we multiply by its transpose to get a dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#get global indices for diagonal\n",
    "gi, lri, lci = dMI.local_diagonal_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#create matrix to store diagonal row\n",
    "dMI_diag=core.DistributedMatrix.empty_like(dMI)\n",
    "dMI_row1=core.DistributedMatrix.empty_like(dMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dgi, dlri, dlci = dMI_diag.local_diagonal_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dMI_diag.local_array[dlri,dlci]=dMI.local_array[lri,lci]\n",
    "#dMI_diag.local_array[0,dlci]=dMI.local_array[lri,lci]\n",
    "#my_diag[comm.rank]=dMI.local_array[lri,lci]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#print(dMI_diag.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a matrix with ones in the first row and zeros elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "ri, ci = dMI_row1.indices()\n",
    "dMI_row1.local_array[:]= ((ri==0).astype(int)).astype(float)\n",
    "#print(dMI_row1.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiply the matrices to get diagonal values on first row of distributed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#dMI_norm = rt.dot(dMI_diag,dMI_row1,transA='T')\n",
    "dMI_norm = rt.dot(dMI_row1,dMI_diag)\n",
    "#dMI_norm = dMI_diag.dot(dMI_row1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "####print(dMI_norm.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiply the matrix with its transpose to get a dense matrix for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import scalapy.routines as rt\n",
    "#dMI_norm=dMI_diag.T*dMI_diag\n",
    "dMI_norm2 = rt.dot(dMI_norm,dMI_norm,transA='T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#print(dMI_norm2.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_diag.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_diag.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_diag.block_shape))\n",
    "#print(dMI_diag.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#blocksize=slice_size\n",
    "#n_jobs_per_rank= (int((b * (b + 1)) / 2))/comm.Get_size()\n",
    "#import collections\n",
    "#for i in range(b):\n",
    "#    #for j in range(i+1,b): # j in range [i,b]\n",
    "#    #idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "#    #srank = idx//n_jobs_per_rank\n",
    "#    lA=\n",
    "#    s_block_shape=np.shape(lA)\n",
    "#    if isinstance(SM[i][j], collections.Iterable):\n",
    "#        lA=local_diag\n",
    "#        s_block_shape=np.shape(lA)\n",
    "#    #broadcast sending ranks block shape to all\n",
    "#    s_block_shape = comm.bcast(s_block_shape, root=srank)   \n",
    "#    dMI_diag.np2self(lA, srow=i*blocksize, scol=0, block_shape=s_block_shape, rank=srank )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "##Array must be 2D for this to work\n",
    "##convert to fortran array indexing to match scalapack functions\n",
    "#global_diag=np.asfortranarray(global_diag)\n",
    "##create global matrix from array on rank0\n",
    "#dMI_diag=core.DistributedMatrix.from_global_array(global_diag,rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_diag.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_diag.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_diag.block_shape))\n",
    "##print(dMI_diag.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#dMI_diag_T=dMI_diag.T\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_diag_T.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_diag_T.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_diag_T.block_shape))\n",
    "#\n",
    "#print(dMI_diag_T.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use scalapack to compute distributed GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#import scalapy.routines as rt\n",
    "##dMI_norm=dMI_diag.T*dMI_diag\n",
    "#dMI_norm = rt.dot(dMI_diag,dMI_diag,transA='T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_norm2.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_norm2.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_norm2.block_shape))\n",
    "#print(dMI_norm2.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the square root of the normalization matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#compute sqrt of each element\n",
    "dMI_norm_square=core.DistributedMatrix.empty_like(dMI)\n",
    "dMI_norm_square.local_array[:] = np.sqrt(dMI_norm2.local_array[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_norm_square.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_norm_square.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_norm_square.block_shape))\n",
    "#print(dMI_norm_square.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can finally compute the norm of the MI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dMI_normed=core.DistributedMatrix.empty_like(dMI)\n",
    "dMI_normed.local_array[:] = dMI.local_array[:] / dMI_norm_square.local_array[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_normed.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_normed.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_normed.block_shape))\n",
    "#print(dMI_normed.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "mi_normed_filename = data_file_path+project_name+'_mi_normed_distributed.scalapack'\n",
    "dMI_normed.to_file(mi_normed_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now compute eigenvalues and eigenvectors of dissimmilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mds(in_mat_file, max_dim, out_file_name, perplexity=30, print_plot=\"True\", dist_method=\"mi\",):\n",
    "#    hdf = pd.HDFStore(in_mat_file)\n",
    "#    if dist_method == \"mi\":\n",
    "#        dlplf = 1 - hdf[\"norm_mi\"]\n",
    "#    elif dist_method == \"euclidean\":\n",
    "#        df = hdf[dist_method]\n",
    "#    else:\n",
    "#        df = 1 - hdf[dist_method]\n",
    "#\n",
    "#    hdf.close()\n",
    "#    n = df.shape[0]\n",
    "#    H = np.eye(n) - np.ones((n, n)) / n\n",
    "#    B = -H.dot(df ** 2).dot(H) / 2\n",
    "#    evals, evecs = eigh(B, eigvals=(n - np.min([n, 200]), n - 1))\n",
    "#    \n",
    "#    idx = np.argsort(evals)[::-1]\n",
    "#    evals = evals[idx]\n",
    "#    evecs = evecs[:, idx]\n",
    "#    evals_pos = evals > 0\n",
    "#    L = np.diag(np.sqrt(evals[evals_pos]))\n",
    "#    V = evecs[:, evals_pos]\n",
    "#    Y = pd.DataFrame(\n",
    "#        data=V.dot(L),\n",
    "#        index=df.index,\n",
    "#        columns=[\"mds_\" + str(x) for x in np.arange(1, L.shape[0] + 1)],\n",
    "#    )\n",
    "#\n",
    "#    Y.to_hdf(out_file_name + \"_reduced.h5\", \"mds\")  # save reduced mi in mds\n",
    "#\n",
    "#    if print_plot == \"True\":\n",
    "#        vis = tsne(\n",
    "#            Y,\n",
    "#            max_dim,\n",
    "#            out_file_name,\n",
    "#            \"mds\",\n",
    "#            perplexity,\n",
    "#            print_plot,\n",
    "#        )\n",
    "#        vis.to_hdf(out_file_name + \"_reduced\", \"mds_tsne\")  # save preview in key \"mds_tsne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Elapsed = 5.076117515563965\n",
      "[stdout:1] Elapsed = 5.073058605194092\n",
      "[stdout:2] Elapsed = 5.073890209197998\n",
      "[stdout:3] Elapsed = 5.0699381828308105\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import time\n",
    "start = time.time()\n",
    "import scalapy.routines as rt\n",
    "\n",
    "n= g_nrows\n",
    "\n",
    "#convert similarity matrix to dissimilarity matrix\n",
    "#df= 1-df\n",
    "MDS= core.DistributedMatrix.empty_like(dMI)\n",
    "MDS.local_array[:]=1.0-dMI_normed.local_array[:]\n",
    "\n",
    "# H = I-Ones/n\n",
    "I= core.DistributedMatrix.identity(n=g_nrows)\n",
    "Ones= core.DistributedMatrix.empty_like(dMI)\n",
    "Ones.local_array[:]=1.0/n\n",
    "H = core.DistributedMatrix.empty_like(dMI)\n",
    "H.local_array[:] = I.local_array[:] - Ones.local_array[:]\n",
    "\n",
    "# B = -H.dot(MDS**2).dot(H)/2\n",
    "negH= core.DistributedMatrix.empty_like(dMI)\n",
    "negH.local_array[:]= -H.local_array[:]\n",
    "MDS2= core.DistributedMatrix.empty_like(dMI)\n",
    "MDS2.local_array[:] = MDS.local_array[:]**2\n",
    "C=rt.dot(negH,MDS2)\n",
    "B = rt.dot(C,H)\n",
    "B.local_array[:]=B.local_array[:]/2.0\n",
    "#dMI_norm=dMI_diag.T*dMI_diag\n",
    "#dMI_norm = rt.dot(dMI_diag,dMI_diag,transA='T')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed = %s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, MDS.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, MDS.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, MDS.block_shape))\n",
    "##print(MDS.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, MDS2.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, MDS2.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, MDS2.block_shape))\n",
    "#print(MDS2.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, B.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, B.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, B.block_shape))\n",
    "#print(B.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Elapsed = 2.6240415573120117\n",
      "[stdout:1] Elapsed = 2.623486042022705\n",
      "[stdout:2] Elapsed = 2.624171733856201\n",
      "[stdout:3] Elapsed = 2.6239771842956543\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import scalapy.routines as rt\n",
    "#compute eigh(B,)\n",
    "#we want to pick out the top 200 eigenvalues/vectors from the matrix\n",
    "#evals, evecs = eigh(B, eigvals=(n - np.min([n, 200]), n - 1))\n",
    "\n",
    "#evals, dZd = rt.eigh(B,overwrite_a=False)\n",
    "#returns same evals np.array to all ranks\n",
    "# and distributed evecs matrix dZd\n",
    "#evals, dZd = rt.eigh(B,overwrite_a=False,eigvals=(n - np.min([n, 200]), n - 1))\n",
    "evals, dZd = rt.eigh(B,eigvals=(n - np.min([n, 200]), n - 1)) #689 seconds!!\n",
    "#evals, dZd = rt.eigh(B) #281.32007026672363 seconds. Not sure why this is less\n",
    "\n",
    "#copy evecs to root\n",
    "evecs = dZd.to_global_array(rank=0)\n",
    "#gZd = dZd.to_global_array(rank=0)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed = %s\" % (end - start))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather the top 200 eigenvalues on a single rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in original dataframe to get labels to attach to results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#get index names from original dataframe\n",
    "import pandas as pd\n",
    "if rank==0:\n",
    "    data_file_path = cwd+'/test_data/inputs/10x/PBMC/3k/pre-processed/'\n",
    "    input_file_name = data_file_path + 'pbmc3k_preprocessed.h5ad'\n",
    "    adf=utils.read_anndata_file(input_file_name)\n",
    "    index=adf.obs.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess the eigenvalues by sorting and removing negative vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 200\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    print(len(idx))\n",
    "    \n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "    evals_pos = evals > 0\n",
    "    L = np.diag(np.sqrt(evals[evals_pos]))\n",
    "    V = evecs[:, evals_pos]\n",
    "    #print(V)\n",
    "    \n",
    "    Y = pd.DataFrame(\n",
    "        data=V.dot(L),\n",
    "        index=index, #need to reattach index names to eigenvectors\n",
    "        columns=[\"mds_\" + str(x) for x in np.arange(1, L.shape[0] + 1)],\n",
    "    )\n",
    "    \n",
    "#Y.to_hdf(out_file_name + \"_reduced.h5\", \"mds\")  # save reduced mi in mds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write reduced data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    data_file_path = cwd+'/test_data/inputs/10x/PBMC/3k/pre-processed/'\n",
    "    out_file_name = data_file_path + 'pbmc3k_preprocessed'    \n",
    "    Y.to_hdf(out_file_name + \"_reduced.h5\", \"mds\")  # save reduced mi in mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "                     mds_1     mds_2     mds_3     mds_4     mds_5     mds_6  \\\n",
      "AAACATACAACCAC-1  0.080218 -0.007323  0.032147 -0.021112 -0.008207 -0.000903   \n",
      "AAACATTGATCAGC-1  0.051539 -0.056262  0.045581  0.053011  0.010254  0.053645   \n",
      "AAACCGTGCTTCCG-1 -0.188360  0.023939  0.013209  0.032328 -0.012209 -0.021553   \n",
      "AAACCGTGTATGCG-1 -0.056562 -0.144185 -0.148594 -0.018133  0.063652  0.010560   \n",
      "AAACGCACTGGTAC-1  0.038719 -0.022159  0.008433  0.000035 -0.009473  0.002272   \n",
      "...                    ...       ...       ...       ...       ...       ...   \n",
      "TTTCGAACTCTCAT-1 -0.208469  0.031346  0.019770  0.031711 -0.039463  0.031550   \n",
      "TTTCTACTGAGGCA-1 -0.007556 -0.000021 -0.039614  0.084780 -0.006493  0.035069   \n",
      "TTTCTACTTCCTCG-1  0.043454  0.139369 -0.093433  0.012428 -0.007409  0.014009   \n",
      "TTTGCATGAGAGGC-1  0.003246  0.097821 -0.129420 -0.020403  0.009517  0.044159   \n",
      "TTTGCATGCCTCAC-1  0.090826  0.018071  0.057012  0.003005  0.021881  0.010015   \n",
      "\n",
      "                     mds_7     mds_8     mds_9    mds_10  ...   mds_191  \\\n",
      "AAACATACAACCAC-1  0.031880 -0.009273 -0.017462  0.049151  ...  0.002239   \n",
      "AAACATTGATCAGC-1  0.024677 -0.021591  0.012128 -0.031019  ... -0.012638   \n",
      "AAACCGTGCTTCCG-1  0.015321 -0.007232  0.006290  0.025058  ...  0.004149   \n",
      "AAACCGTGTATGCG-1 -0.062892 -0.018896 -0.005781 -0.028186  ... -0.009265   \n",
      "AAACGCACTGGTAC-1  0.004931  0.067741  0.030675 -0.047999  ...  0.008916   \n",
      "...                    ...       ...       ...       ...  ...       ...   \n",
      "TTTCGAACTCTCAT-1 -0.059284 -0.015521 -0.034725  0.020430  ...  0.026824   \n",
      "TTTCTACTGAGGCA-1 -0.048476  0.028254 -0.008665 -0.017081  ... -0.028006   \n",
      "TTTCTACTTCCTCG-1  0.036398 -0.064123 -0.027630 -0.017725  ... -0.016676   \n",
      "TTTGCATGAGAGGC-1  0.017281 -0.053414  0.027121  0.002572  ...  0.004310   \n",
      "TTTGCATGCCTCAC-1 -0.029836 -0.027129  0.025564  0.006511  ... -0.002661   \n",
      "\n",
      "                   mds_192   mds_193   mds_194   mds_195   mds_196   mds_197  \\\n",
      "AAACATACAACCAC-1 -0.022240  0.010726 -0.004933 -0.012688  0.003064  0.017539   \n",
      "AAACATTGATCAGC-1  0.021262 -0.008125  0.003422 -0.003511 -0.007451  0.018855   \n",
      "AAACCGTGCTTCCG-1  0.009565  0.003437  0.020055  0.007267  0.027475  0.007106   \n",
      "AAACCGTGTATGCG-1  0.007689 -0.018929  0.010247  0.037014  0.004172 -0.011180   \n",
      "AAACGCACTGGTAC-1  0.007547  0.022570 -0.026644  0.004199  0.014892  0.012679   \n",
      "...                    ...       ...       ...       ...       ...       ...   \n",
      "TTTCGAACTCTCAT-1 -0.002184 -0.004967  0.007321 -0.012372  0.014197  0.000649   \n",
      "TTTCTACTGAGGCA-1  0.010855 -0.018123  0.017103 -0.011716  0.013310  0.013343   \n",
      "TTTCTACTTCCTCG-1 -0.014360 -0.006884 -0.009140  0.004879  0.002407  0.009625   \n",
      "TTTGCATGAGAGGC-1 -0.029850  0.004462 -0.030809  0.040227  0.001213  0.012828   \n",
      "TTTGCATGCCTCAC-1 -0.008603  0.009447 -0.006990  0.019070 -0.015489  0.010414   \n",
      "\n",
      "                   mds_198   mds_199   mds_200  \n",
      "AAACATACAACCAC-1 -0.010703 -0.003451  0.003498  \n",
      "AAACATTGATCAGC-1 -0.001539 -0.013805  0.023951  \n",
      "AAACCGTGCTTCCG-1  0.002376  0.028122  0.001705  \n",
      "AAACCGTGTATGCG-1 -0.011146  0.018554 -0.008425  \n",
      "AAACGCACTGGTAC-1  0.001683  0.018311 -0.026561  \n",
      "...                    ...       ...       ...  \n",
      "TTTCGAACTCTCAT-1  0.006383 -0.019538  0.000683  \n",
      "TTTCTACTGAGGCA-1 -0.008310  0.021434  0.020372  \n",
      "TTTCTACTTCCTCG-1  0.017775 -0.022457 -0.023317  \n",
      "TTTGCATGAGAGGC-1 -0.013609 -0.020714 -0.013362  \n",
      "TTTGCATGCCTCAC-1 -0.017760  0.000414 -0.022229  \n",
      "\n",
      "[2496 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#perplexity=30\n",
    "#max_dim=200\n",
    "#    Y.to_hdf(out_file_name + \"_reduced.h5\", \"mds\")  # save reduced mi in mds\n",
    "#\n",
    "#    if print_plot == \"True\":\n",
    "#        vis = tsne(\n",
    "#            Y,\n",
    "#            max_dim,\n",
    "#            out_file_name,\n",
    "#            \"mds\",\n",
    "#            perplexity,\n",
    "#            print_plot,\n",
    "#        )\n",
    "#        vis.to_hdf(out_file_name + \"_reduced\", \"mds_tsne\")  # save preview in key \"mds_tsne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

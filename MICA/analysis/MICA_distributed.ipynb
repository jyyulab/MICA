{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MPI and Scalapy to distribute all of MICA workflow to work on multiple nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep.py component \\\n",
    "Read input file and slice into manageable sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MICA.lib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython import parallel\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch an ipython parallel cluster\n",
    "Run this on hpc node to launch a cluster with mpi engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this currently needs to be launched from terminal\n",
    "#import subprocess \n",
    "#We need to launch an ipython parallel cluster\n",
    "#!ipcluster start --n=4\n",
    "#subprocess.Popen(['ipcluster','start','--n=4'])\n",
    "#subprocess.Popen(['ipcluster', 'start', '--engines=MPIEngineSetLauncher', '--log-level', 'DEBUG', '--n=4'])\n",
    "#!ipcluster start --engines=MPIEngineSetLauncher --log-level DEBUG --n=4 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a parallel client so that we can use %%px cell magic\n",
    "# With rc and dview, we can interact between mpi ranks and the thread running this notebook\n",
    "rc = ipp.Client()\n",
    "dview = rc[:]\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#load all necessary libraries onto each rank\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "import numba\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy as sci\n",
    "import numpy as np\n",
    "import anndata\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "import fast_histogram\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from MICA.lib import utils\n",
    "from scalapy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] noderome178[9284]: 0/4\n",
      "[stdout:1] noderome178[9285]: 1/4\n",
      "[stdout:2] noderome178[9286]: 2/4\n",
      "[stdout:3] noderome178[9287]: 3/4\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import os\n",
    "import socket\n",
    "#from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "#sys.stdout.write(\n",
    "#    \"Hello, World! I am process %d of %d on %s.\\n\" \n",
    "#    % (rank, size, name))\n",
    "\n",
    "print(\"{host}[{pid}]: {rank}/{size}\".format(\n",
    "    host=socket.gethostname(),\n",
    "    pid=os.getpid(),\n",
    "    rank=comm.rank,\n",
    "    size=comm.size,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#this is a defined elsewhere as a standalone python executable\n",
    "#we are duplicating here to experiment with format for distrubuted computing\n",
    "def prep_dist(input_file, out_name, slice_unit):\n",
    "    \"\"\" Preprocess input file to create sliced matrices.\n",
    "\n",
    "    Reads file into HDF5-format, adds parameters, slices data in file, generates several files with\n",
    "    different combinations of slices.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): path to input text-file\n",
    "        out_name   (str): common rootname of generated output files\n",
    "        slice_unit (int): size of each slice of cell data in input text-file\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    #Read in whole file stored in anndata csr format\n",
    "    adf=utils.read_anndata_file(input_file)\n",
    "    #if adf==None :\n",
    "    #    raise Exception(\"Input file \",input_file,\" not found.\")\n",
    "        \n",
    "    print(\"initial data size=\",adf.shape)\n",
    "\n",
    "    slice_size = int(slice_unit)\n",
    "    \n",
    "    #df = pd.HDFStore(df_file)[\"slice_0\"]\n",
    "    \n",
    "    #compute number of slices needed to break dataset in to slice_size row blocks\n",
    "    b = int(np.ceil(float(adf.shape[0]) / float(slice_size)))\n",
    "    #determine how many digits are in b so we can pad spaces for the string output\n",
    "    digit = int(np.floor(np.log10(b)) + 1)\n",
    "    #loop over slice numbers\n",
    "    for i in range(b):\n",
    "        #slice name is equal to batch index\n",
    "        slice_name = str(i).zfill(digit)\n",
    "        #compute batch row indices\n",
    "        start = i * slice_size\n",
    "        end = np.min([(i + 1) * slice_size, adf.shape[0]])\n",
    "        #copy slice to array of slices\n",
    "        adf_sub=adf[start:end,:]\n",
    "        #write to file so we don't have to keep each slice in memory\n",
    "        output_file_name = out_name + \".slice_\" + slice_name +\".h5ad\"\n",
    "        print(\"output_file_name: \",output_file_name)\n",
    "        adf_sub.write(output_file_name)\n",
    "        \n",
    "    #return nrows and nslices\n",
    "    return adf.shape[0], adf.shape[1], b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#ceb create csr version of numba_histogram2d, also compute_bin with knowledge that minx will always be zero\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_nan_fill(x):\n",
    "    shape = x.shape\n",
    "    x = x.ravel()\n",
    "    x[np.isnan(x)] = 0.0\n",
    "    x = x.reshape(shape)\n",
    "    return x\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_inf_fill(x):\n",
    "    shape = x.shape\n",
    "    x = x.ravel()\n",
    "    x[np.isinf(x)] = 0.0\n",
    "    x = x.reshape(shape)\n",
    "    return x\n",
    "\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def compute_bin_upperbound(x, max, num_bins):\n",
    "    \"\"\" Compute bin index for a give number.\n",
    "        Assume that min is always zero\n",
    "    \"\"\"\n",
    "    # special case to mirror NumPy behavior for last bin\n",
    "    if x == max:\n",
    "        return num_bins - 1 # a_max always in last bin\n",
    "\n",
    "    bin = int(num_bins * x / max)\n",
    "\n",
    "    if bin >= num_bins:\n",
    "        return None\n",
    "    else:\n",
    "        return bin\n",
    "\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def numba_histogram2d_csr(arr1, cols1, arr2, cols2, ncols, num_bins):\n",
    "    \"\"\" Compute the bi-dimensional histogram of two data samples.\n",
    "    Args:\n",
    "        arr1 (array_like, shape (N,)): An array containing the x coordinates of the points to be histogrammed.\n",
    "        arr2 (array_like, shape (N,)): An array containing the y coordinates of the points to be histogrammed.\n",
    "        num_bins (int): int\n",
    "    Return:\n",
    "        hist (2D ndarray)\n",
    "    \"\"\"\n",
    "    #for csr arrays we have to compute zero bins ahead of time \n",
    "        \n",
    "    bin_indices1 = np.zeros((ncols,), dtype=np.int16)\n",
    "    max1 = arr1.max()\n",
    "    #note that bin_indices has same size/indices as full array x and y\n",
    "    for i, x in enumerate(arr1.flat):\n",
    "        #assume zero min\n",
    "        bin_indices1[cols1[i]] = compute_bin_upperbound(x, max1, num_bins)\n",
    "\n",
    "    bin_indices2 = np.zeros((ncols,), dtype=np.int16)\n",
    "    max2 = arr2.max()\n",
    "    for i, y in enumerate(arr2.flat):\n",
    "        #assume zero min\n",
    "        bin_indices2[cols2[i]] = compute_bin_upperbound(y, max2, num_bins)\n",
    "\n",
    "    hist = np.zeros((num_bins, num_bins), dtype=np.int16)\n",
    "    for i, b in enumerate(bin_indices1):\n",
    "        hist[b, bin_indices2[i]] += 1\n",
    "        \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#import numba\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def numba_calc_mi_dis_csr(arr1, cols1, arr2, cols2, bins, m):\n",
    "    \"\"\" Calculates a mutual information distance D(X, Y) = H(X, Y) - I(X, Y) using bin-based method\n",
    "\n",
    "    It takes gene expression data from single cells, and compares them using standard calculation for\n",
    "    mutual information and joint entropy. It builds a 2d histogram, which is used to calculate P(arr1, arr2).\n",
    "\n",
    "    Args:\n",
    "        arr1 (float) nparray of csr: gene expression data for cell 1\n",
    "        cols1 (int): column indices for csr arr1\n",
    "        arr2 (float) nparray of csr: gene expression data for cell 2\n",
    "        cols2 (int): column indices for csr arr\n",
    "        bins           (int): number of bins\n",
    "        m              (int): number of genes\n",
    "    Returns:\n",
    "        a float between 0 and 1\n",
    "    \"\"\"\n",
    "    hist = numba_histogram2d_csr(arr1, cols1, arr2, cols2, m, bins)\n",
    "    \n",
    "    sm = np.sum(hist, axis=1)\n",
    "    tm = np.sum(hist, axis=0)\n",
    "    sm = sm / float(sm.sum())\n",
    "    tm = tm / float(tm.sum())\n",
    "\n",
    "    sm_tm = np.zeros((bins, bins), dtype=np.float32)\n",
    "    for i, s in enumerate(sm):\n",
    "        for j, t in enumerate(tm):\n",
    "            sm_tm[i, j] = s * t\n",
    "\n",
    "    fq = hist / float(m)\n",
    "    div = np.true_divide(fq, sm_tm)\n",
    "    numba_nan_fill(div)\n",
    "    ent = np.log(div)\n",
    "    numba_inf_fill(ent)\n",
    "    agg = np.multiply(fq, ent)\n",
    "    #joint_ent = -np.multiply(fq, numba_inf_fill(np.log(fq))).sum()\n",
    "    #return joint_ent - agg.sum()\n",
    "    return agg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#numba compilation cannot interpret the creation of a 2d array inside of this function so we pass in and out SM_block instead of returning it\n",
    "#import numba\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def process_matrices(Arows,Amat_data,Amat_indptr,Amat_indices,\n",
    "                     Brows,Bmat_data,Bmat_indptr,Bmat_indices,\n",
    "                     num_bins,num_genes,\n",
    "                     SM_block, symmetric=False):\n",
    "    #(mat1.n_obs,mat1.X.data,mat1.X.indptr,mat1.X.indices, mat2.n_obs,mat2.X.data,mat2.X.indptr,mat2.X.indices,SM,num_bins,mat2.n_vars,symmetric)\n",
    "\n",
    "    #Arows = mat1.n_obs\n",
    "    #Brows = mat2.n_obs\n",
    "    #num_genes = mat1.n_vars\n",
    "    \n",
    "    #SM_block = np.ndarray(shape=(Arows,Brows))#, dtype=float, order='F')\n",
    "    #SM_block = SM_block.reshape(Arows,Brows)\n",
    "    for i in range(Arows):\n",
    "        Arowstart = Amat_indptr[i]\n",
    "        Arowend   = Amat_indptr[i+1]\n",
    "        Arow_cols = Amat_indices[Arowstart:Arowend]\n",
    "        Arow_data = Amat_data[Arowstart:Arowend]\n",
    "\n",
    "        Bstart=0\n",
    "        Bend=Brows\n",
    "        #if(symmetric):Bstart=i #upper triangluar\n",
    "        if(symmetric):Bend=i+1 #lower triangular\n",
    "        for j in range(Bstart,Bend): \n",
    "            Browstart = Bmat_indptr[j]\n",
    "            Browend   = Bmat_indptr[j+1]\n",
    "            Brow_cols = Bmat_indices[Browstart:Browend]\n",
    "            Brow_data = Bmat_data[Browstart:Browend]               \n",
    "            SM_block[i,j] = numba_calc_mi_dis_csr(Arow_data, Arow_cols, Brow_data, Brow_cols, num_bins, num_genes)\n",
    "            #SM_block[i*Arows+j] = numba_calc_mi_dis_csr(Arow_data, Arow_cols, Brow_data, Brow_cols, num_bins, num_genes)\n",
    "\n",
    "            \n",
    "    return #SM_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#from mpi4py import MPI\n",
    "from scipy.sparse import csr_matrix\n",
    "#import sys\n",
    "import pandas as pd\n",
    "\n",
    "#create a 2d list to hold blocks of similarity matrix\n",
    "#this should be a global var\n",
    "#SM = [[None for j in range(b)] for i in range(b)] \n",
    "\n",
    "def calc_distance_metric_distributed(in_file_path, project_name, nrows, ncols, nslices, SM):\n",
    "    \n",
    "    \"\"\" Prepares the already sliced input file for further calculation in MICA.\n",
    "    Enters pairs of slices (matrices) into temporary HDF5-format files. It enters them\n",
    "    individually, using their unique key. It also enters the parameter data for every single \n",
    "    pair into the key \"params\", which consists of: [key1, key2, num_bins, num_genes,\n",
    "    pair_index, project_name, num_slices]\n",
    "    Args:\n",
    "        in_file      (str): path to sliced HDF5-format file\n",
    "        project_name (str): project name used to generate path for final outputs\n",
    "        nrows : number of rows in global matrix\n",
    "        ncols : number of vars in global matrix\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #create a 2d list to hold blocks of similarity matrix\n",
    "    #this should be a global var\n",
    "    #SM = [[None for j in range(b)] for i in range(b)] \n",
    "    \n",
    "    \n",
    "    #input file is full input that has been segmented into b blocks of rows\n",
    "    \n",
    "    #nranks would ideally be equal to  b(b+1)/2\n",
    "    comm = MPI.COMM_WORLD\n",
    "    size = comm.Get_size()\n",
    "    myrank = comm.Get_rank()\n",
    "    name = MPI.Get_processor_name()\n",
    "    #sys.stdout.write(\"Hello, World! I am process %d of %d on %s.\\n\" % (myrank, size, name))\n",
    "    \n",
    "    #in_ = pd.HDFStore(in_file, \"r\")  # slice.h5\n",
    "    #b = int(np.ceil(float(nrows) / float(slice_size)))\n",
    "    digit = int(np.floor(np.log10(nslices)) + 1)\n",
    "\n",
    "    num_bins = int(np.floor(nrows ** (1 / 3.0)))  # number of bins\n",
    "    #print(\"bins= \",bins)\n",
    "    #b = in_[\"attr\"].loc[\"slice\", 0]  # number of sliced matrix\n",
    "    b = nslices #number of row blocks (cells)\n",
    "    m = ncols  # number of cols per row (genes)\n",
    "\n",
    "    n_block_comparisons = int((b * (b + 1)) / 2)  # total number of row block comparisons needed to compute entire global similarity matrix\n",
    "    n_jobs_per_rank = n_block_comparisons/size\n",
    "    if (myrank == 0): print(\"block comparsons = %d. jobs per rank = %d\\n\" % (n_block_comparisons, n_jobs_per_rank))\n",
    "\n",
    "    #build list of row block comparisons that current mpi rank will process\n",
    "    myslices=[]\n",
    "    for i in range(b):\n",
    "        for j in range(i,b): # j in range [i,b]\n",
    "            idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "            targetrank = idx//n_jobs_per_rank\n",
    "            if (targetrank == myrank): myslices.append((i,j))            \n",
    "\n",
    "    #from list just generated, only do work assigned to your rank\n",
    "    for index, tuple in enumerate(myslices):\n",
    "        #print(\"tuple=\",tuple)\n",
    "        i=tuple[0] #block row\n",
    "        j=tuple[1] #block col      \n",
    "\n",
    "        #get 1st slice (row block) file\n",
    "        slice_name = str(i).zfill(digit)\n",
    "        ##ceb we only want to read this once per i,j combination\n",
    "        input_file = in_file_path + project_name + \".slice_\" + slice_name +\".h5ad\"\n",
    "        #print(\"infile seg1: \",input_file)\n",
    "        mat1 = utils.read_anndata_file(input_file)\n",
    "    \n",
    "\n",
    "        #get 2nd slice (row block) file\n",
    "        slice_name = str(j).zfill(digit)\n",
    "        input_file = in_file_path + project_name + \".slice_\" + slice_name +\".h5ad\"\n",
    "        #print(\"infile seg2: \",input_file)\n",
    "        mat2 = utils.read_anndata_file(input_file)    \n",
    "\n",
    "        #check to see if block comparison will result in a symmetric SM matrix\n",
    "        # so that we can reduce the number of computations in half\n",
    "        symmetric=False\n",
    "        if i==j: symmetric=True\n",
    "        \n",
    "        print(\"rank: \",myrank,\" comparison between segs:\",i,\" x \",j,\" symmetric=\",symmetric)\n",
    "            \n",
    "        #compute distance metrics between row blocks\n",
    "        \n",
    "        Arows = mat1.n_obs\n",
    "        Brows = mat2.n_obs\n",
    "        num_genes = mat1.n_vars #we will assume Acols==Bcols==num_genes\n",
    "        \n",
    "        #need SM for each block pair    \n",
    "        #creates local array of zeros and assigns to global 2d list\n",
    "        #create matrix of zeros with row order indexing\n",
    "        SM[i][j] = np.zeros(shape=(Arows,Brows), dtype = float, order = 'C')\n",
    " \n",
    "        #This numba function cannot create a numpy array internally so we return SM[i,j] as a variable\n",
    "        process_matrices(mat1.n_obs,mat1.X.data,mat1.X.indptr,mat1.X.indices, \n",
    "                         mat2.n_obs,mat2.X.data,mat2.X.indptr,mat2.X.indices,\n",
    "                         num_bins, num_genes, SM[i][j],\n",
    "                         symmetric #if i==j we can eliminate half of computations\n",
    "                        )\n",
    "\n",
    "        #convert to csr\n",
    "        #we may want to assign this to scalapack distributed matrix here\n",
    "        #SM[i][j]=csr_matrix(SM[i][j])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import os\n",
    "cwd=os.getcwd()\n",
    "if rank==0:\n",
    "    print(cwd)\n",
    "    \n",
    "data_file_path = cwd+'/test_data/inputs/10x/PBMC/3k/pre-processed/'\n",
    "input_file_name = data_file_path + 'pbmc3k_preprocessed.h5ad'\n",
    "project_name = 'pbmc3k'\n",
    "output_file_name = data_file_path+project_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#set slice size (max size of row blocks)\n",
    "slice_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print (input_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Prep_dist() to split file into slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "initial data size= (2496, 10499)\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_0.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_1.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_2.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_3.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_4.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stderr:0] \n",
      "/home/cburdysh/.conda/envs/py36/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n",
      "/home/cburdysh/.conda/envs/py36/lib/python3.6/site-packages/anndata/_core/anndata.py:1192: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if is_string_dtype(df[key]) and not is_categorical(df[key])\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#Run prep.py only on one processor to create the slice files\n",
    "\n",
    "g_nrows=0 #global number of rows (cells)\n",
    "ncols=0\n",
    "nslices=0\n",
    "if rank==0: \n",
    "    #for some unknown reason, running the prep_dist defined in MICA.utils stalls here, although they are identical\n",
    "    g_nrows, ncols, nslices = prep_dist(input_file_name, output_file_name, slice_size)\n",
    "    \n",
    "# this uses the iparallel dview object to distribute these variables from the notebook thread to all mpi ranks\n",
    "# but the ranks can't communicate back to the notebook thread\n",
    "#dview.push(dict(nrows=nrows, ncols=ncols, nslices=nslices))\n",
    "\n",
    "#broadcast resultant variables from root to the other ranks\n",
    "g_nrows = comm.bcast(g_nrows, root=0)\n",
    "ncols = comm.bcast(ncols, root=0)\n",
    "nslices = comm.bcast(nslices, root=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] global nrows, ncols, slices:  2496 10499 5\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print(\"global nrows, ncols, slices: \",g_nrows, ncols, nslices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in anndata preprocessed files (in distributed mode, by node number) and calculate distance metrics between all row pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "block comparsons = 15. jobs per rank = 3\n",
      "\n",
      "rank:  0  comparison between segs: 0  x  0  symmetric= True\n",
      "rank:  0  comparison between segs: 0  x  1  symmetric= False\n",
      "rank:  0  comparison between segs: 0  x  2  symmetric= False\n",
      "rank:  0  comparison between segs: 0  x  3  symmetric= False\n",
      "Elapsed = 26.6834774017334\n",
      "[stdout:1] \n",
      "rank:  1  comparison between segs: 0  x  4  symmetric= False\n",
      "rank:  1  comparison between segs: 1  x  1  symmetric= True\n",
      "rank:  1  comparison between segs: 1  x  2  symmetric= False\n",
      "rank:  1  comparison between segs: 1  x  3  symmetric= False\n",
      "Elapsed = 26.795007944107056\n",
      "[stdout:2] \n",
      "rank:  2  comparison between segs: 1  x  4  symmetric= False\n",
      "rank:  2  comparison between segs: 2  x  2  symmetric= True\n",
      "rank:  2  comparison between segs: 2  x  3  symmetric= False\n",
      "rank:  2  comparison between segs: 2  x  4  symmetric= False\n",
      "Elapsed = 26.59738779067993\n",
      "[stdout:3] \n",
      "rank:  3  comparison between segs: 3  x  3  symmetric= True\n",
      "rank:  3  comparison between segs: 3  x  4  symmetric= False\n",
      "rank:  3  comparison between segs: 4  x  4  symmetric= True\n",
      "Elapsed = 15.428853988647461\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#create a 2d list to hold blocks of similarity matrix\n",
    "#this should be stored in a distributed scalapack matrix\n",
    "b=nslices #row blocks\n",
    "SM = [[None for j in range(b)] for i in range(b)] \n",
    "\n",
    "start = time.time()\n",
    "calc_distance_metric_distributed(data_file_path, project_name, g_nrows, ncols, nslices, SM)\n",
    "end = time.time()\n",
    "print(\"Elapsed = %s\" % (end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px \n",
    "###from scipy.sparse import csr_matrix #may not use csr as it complicates copy to distributed scalapack and is not used in scalapack apparently\n",
    "#import collections\n",
    "#for i in range(b):\n",
    "#    for j in range(i,b):\n",
    "#        if isinstance(SM[i][j], collections.Iterable):\n",
    "#            #print(\"Rank:\",rank, \" SM[\",i,\"][\",j,\"]=\",SM[i][j])\n",
    "#            print(\"SM[\",i,\"][\",j,\"]=\",SM[i][j],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create distributed matrix for scalapack and copy distributed blocks into object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy lower triangular transpose to upper triangular for diagonal blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "##from scipy.sparse import csr_matrix #may not use csr as it complicates copy to distributed scalapack and is not used in scalapack apparently\n",
    "import collections\n",
    "for i in range(b):\n",
    "    for j in range(i,b):\n",
    "        if isinstance(SM[i][j], collections.Iterable):\n",
    "            if i==j: #copy lower triangular transpose to upper triangular \n",
    "                for ii in range(SM[i][j].shape[0]):\n",
    "                    for jj in range(ii+1,SM[i][j].shape[1]):\n",
    "                        (SM[i][j])[ii,jj]=(SM[i][j])[jj,ii]\n",
    "                #print(\"Rank:\",rank, \" SM[\",i,\"][\",j,\"]=\",SM[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate a global array with all of the MI data from each rank\n",
    "\n",
    "Preferably, we would like each rank to contribute of their block MI matrices to the global matrix,\n",
    "but currently the distributed global matrix has to be constructed from a global (not distributed) array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, each rank loads it's matrix contribution into a global block matrix, \\\n",
    "And then we use MPI to transfer these contributions to rank 0 from all other ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy SM data into global distributed matrix and then write to file?\n",
    "\n",
    "#then we can read that file into the Scalapack block cyclic matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] PR= 2 PC= 2\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#test to distribute matrix from local blocks rather than global array\n",
    "from scalapy import blacs\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from mpi4py import MPI\n",
    "from scalapy import core\n",
    "import scalapy.routines as rt\n",
    "\n",
    "#distribute MI components to ranks as scalapack distributed matrix\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size #total number of ranks\n",
    "\n",
    "global_num_rows =g_nrows\n",
    "global_num_cols =g_nrows\n",
    "local_num_rows =g_nrows/b\n",
    "\n",
    "block_size=64 #default is 32\n",
    "\n",
    "#Define process grid with process rows and process cols\n",
    "#We'll use a 2d process grid to distribute blocks so we want to have num_ranks divisible by 2\n",
    "assert((size % 2)==0)\n",
    "#ideally we would like BR and BC to the square root of the num_ranks to get a square process matrix\n",
    "PR=int(np.sqrt(size))\n",
    "PC=PR\n",
    "\n",
    "#if we can't create a square matrix, get next best dimensions\n",
    "if PR*PR!=size:\n",
    "    PC=size//PR\n",
    "if rank==0:\n",
    "    print(\"PR=\",PR, \"PC=\",PC)\n",
    "\n",
    "#sets default context and block_shape\n",
    "core.initmpi([PR, PC],block_shape=[block_size,block_size])\n",
    "#convert to fortran array indexing to match scalapack functions\n",
    "#create global matrix from array on rank0\n",
    "dMI2=core.DistributedMatrix(global_shape=[g_nrows,g_nrows],dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI2 = %s' % (rank, dMI2.global_shape))\n",
    "#print ('rank %d has local_shape of dMI2 = %s' % (rank, dMI2.local_shape))\n",
    "#print ('rank %d has block_shape of dMI2 = %s' % (rank, dMI2.block_shape))\n",
    "##print(dMI2.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#blocksize=slice_size\n",
    "#testrank=3\n",
    "#testmat=np.zeros(shape=(4,4))\n",
    "#if comm.rank==testrank:\n",
    "#    testmat=SM[3][3]\n",
    "#    #testmat=np.zeros(shape=(500,500))\n",
    "#    s_block_shape=np.shape(testmat)\n",
    "#else:\n",
    "#    testmat=np.zeros(shape=(4,4))\n",
    "#    s_block_shape=np.shape(testmat)\n",
    "\n",
    "#s_block_shape = comm.bcast(s_block_shape, root=testrank)   \n",
    "#copy_from_np(dMI2, testmat, asrow=0, anrow=None, ascol=0, ancol=None, srow=0, scol=0, block_shape=s_block_shape, rank=testrank) #all ranks works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy each SM block submatrix to distributed block cyclic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "blocksize=slice_size\n",
    "n_jobs_per_rank= (int((b * (b + 1)) / 2))/comm.Get_size()\n",
    "import collections\n",
    "\n",
    "for i in range(b):\n",
    "    for j in range(i,b): # j in range [i,b]\n",
    "        idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "        srank = idx//n_jobs_per_rank\n",
    "        lA=np.zeros(shape=(2,2))\n",
    "        s_block_shape=np.shape(lA)\n",
    "        if isinstance(SM[i][j], collections.Iterable):\n",
    "            lA=SM[i][j]\n",
    "            s_block_shape=np.shape(lA)\n",
    "            #print(\"copy SM[\",i,j,\"] shape: \",s_block_shape)\n",
    "        #broadcast sending ranks block shape to all\n",
    "        s_block_shape = comm.bcast(s_block_shape, root=srank)   \n",
    "        dMI2.np2self(lA, srow=i*blocksize, scol=j*blocksize, block_shape=s_block_shape, rank=srank )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI2 = %s' % (rank, dMI2.global_shape))\n",
    "#print ('rank %d has local_shape of dMI2 = %s' % (rank, dMI2.local_shape))\n",
    "#print ('rank %d has block_shape of dMI2 = %s' % (rank, dMI2.block_shape))\n",
    "#print(dMI2.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy transpose of blocks to fill upper triangular distributed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "blocksize=slice_size\n",
    "n_jobs_per_rank= (int((b * (b + 1)) / 2))/comm.Get_size()\n",
    "import collections\n",
    "\n",
    "for i in range(b):\n",
    "    for j in range(i+1,b): # j in range [i,b]\n",
    "        idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "        srank = idx//n_jobs_per_rank\n",
    "        lA=np.zeros(shape=(2,2))\n",
    "        s_block_shape=np.shape(lA)\n",
    "        if isinstance(SM[i][j], collections.Iterable):\n",
    "            lA=np.transpose(SM[i][j])\n",
    "            s_block_shape=np.shape(lA)\n",
    "            #print(\"copy SM[\",j,i,\"] shape: \",s_block_shape)\n",
    "        #broadcast sending ranks block shape to all\n",
    "        s_block_shape = comm.bcast(s_block_shape, root=srank)   \n",
    "        dMI2.np2self(lA, srow=j*blocksize, scol=i*blocksize, block_shape=s_block_shape, rank=srank )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to also fill in empty symmetric upper triangular portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though this is a symmetric matrix, for further processing, we need to copy block data to rest of matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "rank 0 has global_shape of dMI2 = (2496, 2496)\n",
      "rank 0 has local_shape of dMI2 = (1280, 1280)\n",
      "rank 0 has block_shape of dMI2 = (64, 64)\n",
      "[[0.33805914 0.066332   0.05676848 ... 0.05752099 0.04267657 0.0668972 ]\n",
      " [0.066332   0.45212405 0.06047588 ... 0.06336014 0.04781611 0.06866279]\n",
      " [0.05676848 0.06047588 0.39173321 ... 0.05280795 0.04746441 0.05657429]\n",
      " ...\n",
      " [0.05752099 0.06336014 0.05280795 ... 0.28514319 0.05104461 0.05835302]\n",
      " [0.04267657 0.04781611 0.04746441 ... 0.05104461 0.21451758 0.04515555]\n",
      " [0.0668972  0.06866279 0.05657429 ... 0.05835302 0.04515555 0.31753523]]\n",
      "[stdout:1] \n",
      "rank 1 has local_shape of dMI2 = (1280, 1216)\n",
      "rank 1 has block_shape of dMI2 = (64, 64)\n",
      "[[0.04636978 0.05072926 0.05500222 ... 0.05035315 0.06835701 0.06148071]\n",
      " [0.05683409 0.05903542 0.05992358 ... 0.06264001 0.07618326 0.06856185]\n",
      " [0.04752714 0.04735661 0.05504157 ... 0.07159437 0.06133829 0.05748736]\n",
      " ...\n",
      " [0.04205094 0.0458999  0.04639795 ... 0.0493155  0.05951592 0.05640946]\n",
      " [0.0368094  0.03827015 0.03748397 ... 0.04050844 0.04859252 0.0417306 ]\n",
      " [0.04546185 0.04896738 0.05036232 ... 0.05157533 0.07009173 0.06224821]]\n",
      "[stdout:2] \n",
      "rank 2 has local_shape of dMI2 = (1216, 1280)\n",
      "rank 2 has block_shape of dMI2 = (64, 64)\n",
      "[[0.04636978 0.05683409 0.04752714 ... 0.04205094 0.0368094  0.04546185]\n",
      " [0.05072926 0.05903542 0.04735661 ... 0.0458999  0.03827015 0.04896738]\n",
      " [0.05500222 0.05992358 0.05504157 ... 0.04639795 0.03748397 0.05036232]\n",
      " ...\n",
      " [0.05035315 0.06264001 0.07159437 ... 0.0493155  0.04050844 0.05157533]\n",
      " [0.06835701 0.07618326 0.06133829 ... 0.05951592 0.04859252 0.07009173]\n",
      " [0.06148071 0.06856185 0.05748736 ... 0.05640946 0.0417306  0.06224821]]\n",
      "[stdout:3] \n",
      "rank 3 has local_shape of dMI2 = (1216, 1216)\n",
      "rank 3 has block_shape of dMI2 = (64, 64)\n",
      "[[0.35717489 0.04458413 0.0649855  ... 0.04528153 0.05382182 0.05145039]\n",
      " [0.04458413 0.2980473  0.04995458 ... 0.04585446 0.05437559 0.0483851 ]\n",
      " [0.0649855  0.04995458 0.3907872  ... 0.04991302 0.05633744 0.0502936 ]\n",
      " ...\n",
      " [0.04528153 0.04585446 0.04991302 ... 0.3529249  0.05757312 0.05081421]\n",
      " [0.05382182 0.05437559 0.05633744 ... 0.05757312 0.38132678 0.06493108]\n",
      " [0.05145039 0.0483851  0.0502936  ... 0.05081421 0.06493108 0.32226236]]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0: \n",
    "    print ('rank %d has global_shape of dMI2 = %s' % (rank, dMI2.global_shape))\n",
    "print ('rank %d has local_shape of dMI2 = %s' % (rank, dMI2.local_shape))\n",
    "print ('rank %d has block_shape of dMI2 = %s' % (rank, dMI2.block_shape))\n",
    "#print(dMI2.local_array[0:20,0:20])\n",
    "print(dMI2.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redistribute for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "###lr=[range(4)]\n",
    "###lc=[range(4)]\n",
    "####Check to see of data was transferred to rank 0\n",
    "#if rank==0:\n",
    "#    for i in range(b):\n",
    "#            for j in range(i,b): # j in range [i,b]\n",
    "#                #print(\"SM[\",i,j,\"] \",np.shape(SM[i][i]))\n",
    "#                print(\"SM[\",i,j,\"] \",SM[i][j])\n",
    "##                print( (SM[i][j])[lr,lc] )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#from inspect import getmembers, isfunction, ismodule\n",
    "#if rank == 0:\n",
    "#    print([o[0] for o in getmembers(scalapy) if ismodule(o[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#from inspect import getmembers, isfunction, ismodule\n",
    "#import scalapy\n",
    "#if rank==0:\n",
    "#    print(getmembers(scalapy.blacs, isfunction))\n",
    "#    #print(getmembers(scalapy.routines, isfunction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#total number of global blocks = total number of block comparisons\n",
    "#Should be greater than number of ranks to improve load balancing\n",
    "# b is number of slices (blocks of rows) original data has been discretized into.\n",
    "#The size of these blocks can be variable\n",
    "#global_number_of_matrix_blocks= int((b * (b + 1)) / 2) \n",
    "\n",
    "#We'll use a 2d process grid to distribute blocks so we want to have num_ranks divisivle by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bcast SM[i][j] from each rank to root rank 0 so that we can load global matrix array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert array into distributed block cyclic global matrix\n",
    "Simply load a numpy vector with data from each block matrix to use in scalapak distributed matrix initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#Write MI matrix to file\n",
    "mi_filename = data_file_path+project_name+'_mi_distributed.scalapack'\n",
    "dMI2.to_file(mi_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#Read MI matrix from file\n",
    "#mi_filename = data_file_path+project_name+'_mi_distributed.scalapack'\n",
    "#dMI.from_file(mi_filename, global_shape=[g_nrows,g_nrows], dtype=np.float64, block_shape=[block_size,block_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI = %s' % (rank, dMI2.global_shape))\n",
    "#print ('rank %d has local_shape of dMI = %s' % (rank, dMI2.local_shape))\n",
    "#print ('rank %d has block_shape of dMI = %s' % (rank, dMI2.block_shape))\n",
    "#print(dMI2.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to create a matrix with the diagonal as the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#Array must be 2D for this to work\n",
    "#convert to fortran array indexing to match scalapack functions\n",
    "global_diag=np.asfortranarray(global_diag)\n",
    "#create global matrix from array on rank0\n",
    "dMI_diag=core.DistributedMatrix.from_global_array(global_diag,rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_diag.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_diag.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_diag.block_shape))\n",
    "##print(dMI_diag.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#dMI_diag_T=dMI_diag.T\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_diag_T.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_diag_T.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_diag_T.block_shape))\n",
    "#\n",
    "#print(dMI_diag_T.local_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use scalapack to compute distributed GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import scalapy.routines as rt\n",
    "#dMI_norm=dMI_diag.T*dMI_diag\n",
    "dMI_norm = rt.dot(dMI_diag,dMI_diag,transA='T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_norm.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_norm.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_norm.block_shape))\n",
    "#print(dMI_norm.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#compute sqrt of each element\n",
    "dMI_norm_square=core.DistributedMatrix.empty_like(dMI)\n",
    "dMI_norm_square.local_array[:] = np.sqrt(dMI_norm.local_array[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#if rank==0: \n",
    "#    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_norm_square.global_shape))\n",
    "#print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_norm_square.local_shape))\n",
    "#print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_norm_square.block_shape))\n",
    "#print(dMI_norm_square.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dMI_normed=core.DistributedMatrix.empty_like(dMI)\n",
    "dMI_normed.local_array[:] = dMI.local_array[:] / dMI_norm_square.local_array[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if rank==0: \n",
    "    print ('rank %d has global_shape of dMI_diag = %s' % (rank, dMI_normed.global_shape))\n",
    "print ('rank %d has local_shape of dMI_diag = %s' % (rank, dMI_normed.local_shape))\n",
    "print ('rank %d has block_shape of dMI_diag = %s' % (rank, dMI_normed.block_shape))\n",
    "print(dMI_normed.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "mi_normed_filename = data_file_path+project_name+'_mi_normed_distributed.scalapack'\n",
    "dMI_normed.to_file(mi_normed_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now compute eigenvalues and eigenvectors of dissimmilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mds(in_mat_file, max_dim, out_file_name, perplexity=30, print_plot=\"True\", dist_method=\"mi\",):\n",
    "#    hdf = pd.HDFStore(in_mat_file)\n",
    "#    if dist_method == \"mi\":\n",
    "#        dlplf = 1 - hdf[\"norm_mi\"]\n",
    "#    elif dist_method == \"euclidean\":\n",
    "#        df = hdf[dist_method]\n",
    "#    else:\n",
    "#        df = 1 - hdf[dist_method]\n",
    "#\n",
    "#    hdf.close()\n",
    "#    n = df.shape[0]\n",
    "#    H = np.eye(n) - np.ones((n, n)) / n\n",
    "#    B = -H.dot(df ** 2).dot(H) / 2\n",
    "#    evals, evecs = eigh(B, eigvals=(n - np.min([n, 200]), n - 1))\n",
    "#    \n",
    "#    idx = np.argsort(evals)[::-1]\n",
    "#    evals = evals[idx]\n",
    "#    evecs = evecs[:, idx]\n",
    "#    evals_pos = evals > 0\n",
    "#    L = np.diag(np.sqrt(evals[evals_pos]))\n",
    "#    V = evecs[:, evals_pos]\n",
    "#    Y = pd.DataFrame(\n",
    "#        data=V.dot(L),\n",
    "#        index=df.index,\n",
    "#        columns=[\"mds_\" + str(x) for x in np.arange(1, L.shape[0] + 1)],\n",
    "#    )\n",
    "#\n",
    "#    Y.to_hdf(out_file_name + \"_reduced.h5\", \"mds\")  # save reduced mi in mds\n",
    "#\n",
    "#    if print_plot == \"True\":\n",
    "#        vis = tsne(\n",
    "#            Y,\n",
    "#            max_dim,\n",
    "#            out_file_name,\n",
    "#            \"mds\",\n",
    "#            perplexity,\n",
    "#            print_plot,\n",
    "#        )\n",
    "#        vis.to_hdf(out_file_name + \"_reduced\", \"mds_tsne\")  # save preview in key \"mds_tsne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import scalapy.routines as rt\n",
    "\n",
    "n= g_nrows\n",
    "\n",
    "#convert similarity matrix to dissimilarity matrix\n",
    "#df= 1-df\n",
    "MDS= core.DistributedMatrix.empty_like(dMI)\n",
    "MDS.local_array[:]=1.0-dMI_normed.local_array[:]\n",
    "\n",
    "# H = I-Ones/n\n",
    "I= core.DistributedMatrix.identity(n=g_nrows)\n",
    "Ones= core.DistributedMatrix.empty_like(dMI)\n",
    "Ones.local_array[:]=1.0/n\n",
    "H = core.DistributedMatrix.empty_like(dMI)\n",
    "H.local_array[:] = I.local_array[:] - Ones.local_array[:]\n",
    "\n",
    "# B = -H.dot(MDS**2).dot(H)/2\n",
    "negH= core.DistributedMatrix.empty_like(dMI)\n",
    "negH.local_array[:]= -H.local_array[:]\n",
    "MDS2= core.DistributedMatrix.empty_like(dMI)\n",
    "MDS2.local_array[:] = MDS.local_array[:]**2\n",
    "C=rt.dot(negH,MDS2)\n",
    "B = rt.dot(C,H)\n",
    "B.local_array[:]=B.local_array[:]/2.0\n",
    "#dMI_norm=dMI_diag.T*dMI_diag\n",
    "#dMI_norm = rt.dot(dMI_diag,dMI_diag,transA='T')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if rank==0: \n",
    "    print ('rank %d has global_shape of dMI_diag = %s' % (rank, MDS.global_shape))\n",
    "print ('rank %d has local_shape of dMI_diag = %s' % (rank, MDS.local_shape))\n",
    "print ('rank %d has block_shape of dMI_diag = %s' % (rank, MDS.block_shape))\n",
    "print(MDS.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if rank==0: \n",
    "    print ('rank %d has global_shape of dMI_diag = %s' % (rank, MDS2.global_shape))\n",
    "print ('rank %d has local_shape of dMI_diag = %s' % (rank, MDS2.local_shape))\n",
    "print ('rank %d has block_shape of dMI_diag = %s' % (rank, MDS2.block_shape))\n",
    "print(MDS2.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if rank==0: \n",
    "    print ('rank %d has global_shape of dMI_diag = %s' % (rank, B.global_shape))\n",
    "print ('rank %d has local_shape of dMI_diag = %s' % (rank, B.local_shape))\n",
    "print ('rank %d has block_shape of dMI_diag = %s' % (rank, B.block_shape))\n",
    "print(B.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import scalapy.routines as rt\n",
    "#compute eigh(B,)\n",
    "#we want to pick out the top 200 eigenvalues/vectors from the matrix\n",
    "#evals, evecs = eigh(B, eigvals=(n - np.min([n, 200]), n - 1))\n",
    "evals, dZd = rt.eigh(B,overwrite_a=False)\n",
    "evecs = dZd.to_global_array(rank=0)\n",
    "#gZd = dZd.to_global_array(rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#import scalapy.routines as rt\n",
    "##obtain dissimilarity matrix\n",
    "#n= g_nrows\n",
    "##MDS=1-dMI\n",
    "#MDS= core.DistributedMatrix.empty_like(dMI)\n",
    "##MDS.local_array[:]=1.0-dMI.local_Array[:]\n",
    "#I= core.DistributedMatrix.identity(n=g_nrows)\n",
    "#Ones= core.DistributedMatrix.empty_like(dMI)\n",
    "#Ones.local_array[:]=1.0/n\n",
    "##create arrays\n",
    "##create identity matrix and then subtract 1/n from all entries\n",
    "#H= np.eye(n) - np.ones((n, n)) / n\n",
    "#H= I - Ones\n",
    "#B= -H.dot(df ** 2).dot(H) / 2\n",
    "#\n",
    "##compute eigh(B,)\n",
    "##we want to pick out the top 200 eigenvalues/vectors from the matrix\n",
    "##evals, evecs = eigh(B, eigvals=(n - np.min([n, 200]), n - 1))\n",
    "#evals, dZd = rt.eigh(B,overwrite_a=False)\n",
    "#evecs = dZd.to_global_array(rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#dgi, dlri, dlci = diagMI.local_diagonal_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#gi, lri, lci = dMI.local_diagonal_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#diagMI.local_array[lri,0] = dMI.local_array[lri,lci]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#print ('rank %d has local_shape of dMI = %s' % (rank, diagMI.local_shape))\n",
    "#print ('rank %d has block_shape of dMI = %s' % (rank, diagMI.block_shape))\n",
    "#print(diagMI.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#diagMI_dot = dot(dMI_diag,dMI_diag,transB='T')\n",
    "#diagMI_dot = dMI_diag.T * dMI_diag\n",
    "\n",
    "#diagMI_dot.local_array[lri,lci] = 1.0/np.sqrt(diagMI_dot.local_array[lri,lci])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#print(diagMI_dot.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#print(diagMI_dot.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#diagMI_normed = dMI*diagMI_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#print(dMI_normed.local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "##lr=[range(2400,2495)]\n",
    "##lc=[range(2400,2495)]\n",
    "#print(len(dMI.local_array[lri,lci]))\n",
    "#diag=dMI.local_array[lri,lci]\n",
    "#print(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#dMI_dot=dot(dMI,dMI,transB='T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#dMI_dot.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#dgi, dlri, dlci = dMI_dot.local_diagonal_indices()\n",
    "#print(dMI_dot.local_array[dlri,dlci])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#Note these values are in mixed order due to block cyclic partition. last rank last block does not correspond to last values in global diagonal\n",
    "#print(diag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    nnzgZd=\n",
    "    print(len(gZd))\n",
    "    print(gZd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute eigenvalues,vectors\n",
    "#evals1, evecs1 = scalapy.eigh(dMI, overwrite_a=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write distributed matrix to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test function to sort out copy_from_np\n",
    "%%px\n",
    "from mpi4py import MPI\n",
    "from scalapy import *\n",
    "\n",
    "def _chk_2d_size(shape, positive=True):\n",
    "    # Check that the shape describes a valid 2D grid. Zero shape not allowed when positive = True.\n",
    "    if len(shape) != 2:\n",
    "        return False\n",
    "    if positive:\n",
    "        if shape[0] <= 0 or shape[1] <= 0:\n",
    "            return False\n",
    "    else:\n",
    "        if shape[0] < 0 or shape[1] < 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def copy_from_np(dmat, a, asrow=0, anrow=None, ascol=0, ancol=None, srow=0, scol=0, block_shape=None, rank=0):\n",
    "        ## copy a section of a numpy array a[asrow:asrow+anrow, ascol:ascol+ancol] to self[srow:srow+anrow, scol:scol+ancol], once per block_shape\n",
    "\n",
    "        Nrow, Ncol = dmat.global_shape\n",
    "        srow = srow if srow >= 0 else srow + Nrow\n",
    "        srow = max(0, srow)\n",
    "        srow = min(srow, Nrow)\n",
    "        scol = scol if scol >= 0 else scol + Ncol\n",
    "        scol = max(0, scol)\n",
    "        scol = min(scol, Ncol)\n",
    "        if dmat.context.mpi_comm.rank == rank:\n",
    "            if not (a.ndim == 1 or a.ndim == 2):\n",
    "                raise ScalapyException('Unsupported high dimensional array.')\n",
    "\n",
    "            a = np.asfortranarray(a.astype(dmat.dtype)) # type conversion\n",
    "            a = a.reshape(-1, a.shape[-1]) # reshape to two dimensional\n",
    "            am, an = a.shape\n",
    "            asrow = asrow if asrow >= 0 else asrow + am\n",
    "            asrow = max(0, asrow)\n",
    "            asrow = min(asrow, am)\n",
    "            ascol = ascol if ascol >= 0 else ascol + an\n",
    "            ascol = max(0, ascol)\n",
    "            ascol = min(ascol, an)\n",
    "            m = am - asrow if anrow is None else anrow\n",
    "            m = max(0, m)\n",
    "            m = min(m, am - asrow, Nrow - srow)\n",
    "            n = an - ascol if ancol is None else ancol\n",
    "            n = max(0, n)\n",
    "            n = min(n, an - ascol, Ncol - scol)\n",
    "        else:\n",
    "            m, n = 1, 1\n",
    "\n",
    "        asrow = dmat.context.mpi_comm.bcast(asrow, root=rank)\n",
    "        ascol = dmat.context.mpi_comm.bcast(ascol, root=rank)\n",
    "        #print(\"asrow ascol=\",asrow,ascol)\n",
    "        #get sending ranks size\n",
    "        m = dmat.context.mpi_comm.bcast(m, root=rank) # number of rows to copy\n",
    "        n = dmat.context.mpi_comm.bcast(n, root=rank) # number of columes to copy\n",
    "        #print(\"m,n=\",m,n)\n",
    "        if m == 0 or n == 0:\n",
    "            return dmat\n",
    "\n",
    "        block_shape = dmat.block_shape if block_shape is None else block_shape\n",
    "        if not _chk_2d_size(block_shape):\n",
    "            raise ScalapyException(\"Invalid block_shape\")\n",
    "        #print(\"block_shape=\",block_shape)\n",
    "        bm, bn = block_shape\n",
    "        br = blockcyclic.num_blocks(m, bm) # number of blocks for row\n",
    "        bc = blockcyclic.num_blocks(n, bn) # number of blocks for column\n",
    "        rm = m - (br - 1) * bm # remained number of rows of the last block\n",
    "        rn = n - (bc - 1) * bn # remained number of columes of the last block\n",
    "        #print(\"bm,bn=\",bm,bn)\n",
    "\n",
    "        # due to bugs in scalapy, it is needed to first init an process context here\n",
    "        ProcessContext([1, dmat.context.mpi_comm.size], comm=dmat.context.mpi_comm) # process context\n",
    "        #print(\"br,bc=\",br,bc)\n",
    "        \n",
    "        for bri in range(br):\n",
    "            M = bm if bri != br - 1 else rm\n",
    "            for bci in range(bc):\n",
    "                N = bn if bci != bc - 1 else rn\n",
    "                if dmat.context.mpi_comm.rank == rank:\n",
    "                    #print(\"main rank=\",dmat.context.mpi_comm.rank,\" bm bn: \",bm,bn,\" bri bci: \",bri,bci,\"srow scol: \",srow,scol)\n",
    "                    pc = ProcessContext([1, 1], comm=MPI.COMM_SELF) # process context\n",
    "                    desc = dmat.desc\n",
    "                    desc[1] = pc.blacs_context #new context containing only this rank\n",
    "                    desc[2], desc[3] = a.shape\n",
    "                    desc[4], desc[5] = a.shape\n",
    "                    desc[8] = a.shape[0]\n",
    "                    #print(\"descA= \",desc)\n",
    "                    #print(\"descB= \",dmat.desc)\n",
    "\n",
    "                    args = [M, N, a,                              asrow+1+bm*bri, ascol+1+bn*bci, desc,   \n",
    "                            dmat._local_array, srow+1+bm*bri, scol+1+bn*bci, dmat.desc, dmat.context.blacs_context]\n",
    "                else:#these processes do not own submatrix, but must still call function\n",
    "                    #print(\"other rank=\",dmat.context.mpi_comm.rank,\" bm bn: \",bm,bn,\" bri bci: \",bri,bci,\"srow scol: \",srow,scol)\n",
    "                    desc = np.zeros(9, dtype=np.int32)\n",
    "                    desc[1] = -1           \n",
    "                    #print(\"descA= \",desc)\n",
    "                    #print(\"descB= \",dmat.desc)\n",
    "                    args = [M, N, np.zeros(1, dtype=dmat.dtype) , asrow+1+bm*bri, ascol+1+bn*bci, desc,   \n",
    "                            dmat._local_array, srow+1+bm*bri, scol+1+bn*bci, dmat.desc, dmat.context.blacs_context]\n",
    "                from scalapy import lowlevel as ll\n",
    "                call_table = {'S': (ll.psgemr2d, args),\n",
    "                              'D': (ll.pdgemr2d, args),\n",
    "                              'C': (ll.pcgemr2d, args),\n",
    "                              'Z': (ll.pzgemr2d, args)}\n",
    "                func, args = call_table[dmat.sc_dtype]\n",
    "                #print (args)\n",
    "                func(*args)\n",
    "        return dmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##formerly used to copy to a global array\n",
    "%%px\n",
    "#copy SM[i][j] from each rank to root rank 0\n",
    "n_jobs_per_rank= (int((b * (b + 1)) / 2))/comm.Get_size()\n",
    "import collections\n",
    "for i in range(b):\n",
    "    for j in range(i,b): # j in range [i,b]\n",
    "        idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "        srank = idx//n_jobs_per_rank\n",
    "        if rank!=0 and isinstance(SM[i][j], collections.Iterable):\n",
    "            #print(\"send[\",i,j,\"] tag: \",idx)\n",
    "            comm.send(SM[i][j],dest=0,tag=idx)\n",
    "        elif rank==0 and not isinstance(SM[i][j], collections.Iterable):\n",
    "            #MB=np.zeros(shape=(Arows,Brows), dtype = float, order = 'C')\n",
    "            MB=comm.recv( source=srank, tag=idx)\n",
    "            SM[i][j]=MB\n",
    "            #print(\"recv[\",i,j,\"] tag: \",idx,\" shape: \",MB.shape, SM[i][j])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and fill distributed matrix from global array\n",
    "%%px\n",
    "from scalapy import blacs\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from mpi4py import MPI\n",
    "from scalapy import core\n",
    "import scalapy.routines as rt\n",
    "\n",
    "#distribute MI components to ranks as scalapack distributed matrix\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size #total number of ranks\n",
    "\n",
    "global_num_rows =g_nrows\n",
    "global_num_cols =g_nrows\n",
    "local_num_rows =g_nrows/b\n",
    "\n",
    "block_size=64 #default is 32\n",
    "\n",
    "#Define process grid with process rows and process cols\n",
    "#We'll use a 2d process grid to distribute blocks so we want to have num_ranks divisible by 2\n",
    "assert((size % 2)==0)\n",
    "#ideally we would like BR and BC to the square root of the num_ranks to get a square process matrix\n",
    "PR=int(np.sqrt(size))\n",
    "PC=PR\n",
    "\n",
    "#if we can't create a square matrix, get next best dimensions\n",
    "if PR*PR!=size:\n",
    "    PC=size//PR\n",
    "if rank==0:\n",
    "    print(\"PR=\",PR, \"PC=\",PC)\n",
    "\n",
    "#create mpi comm context to send to blacs communicator    \n",
    "#process context\n",
    "#do I need this?\n",
    "#cntxt = blacs.sys2blacs_handle(comm)\n",
    "#initialize a process grid to distribute matrix blocks to.\n",
    "#blacs.gridinit(ctxt,order='R',BR,BC)\n",
    "#blacs.gridinit(pc,order='R',BR,BC)\n",
    "#core.initmpi([PR, PC], block_shape=[local_num_rows,local_num_rows])\n",
    "#instead of creating a global array on every processor, we want to load a \n",
    "#global array with local data\n",
    "#MI=core.DistributedMatrix(global_shape=[global_num_rows,global_num_cols])\n",
    "\n",
    "\n",
    "core.initmpi([PR, PC],block_shape=[block_size,block_size])\n",
    "\n",
    "#convert to fortran array indexing to match scalapack functions\n",
    "global_MI=np.asfortranarray(global_MI)\n",
    "#create global matrix from array on rank0\n",
    "dMI=core.DistributedMatrix.from_global_array(global_MI,rank=0)\n",
    "\n",
    "#Array must be 2D, so this fails\n",
    "#convert to fortran array indexing to match scalapack functions\n",
    "#global_diag=np.asfortranarray(global_diag)\n",
    "#create global matrix from array on rank0\n",
    "#dMI_diag=core.DistributedMatrix.from_global_array(global_diag,rank=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#blacs.gridinit(ctxt, b, b)\n",
    "#ranklist = [(0, 0), \n",
    "#            (0, 1), (1, 1),\n",
    "#            (0, 2), (1, 2), (2, 2),\n",
    "#            (0, 3), (1, 3), (2, 3), (3, 3),\n",
    "#            (0, 4), (1, 4), (2, 4), (3, 4), (4, 4)\n",
    "#           ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

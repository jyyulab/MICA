{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MPI and Scalapy to distribute all of MICA workflow to work on multiple nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep.py component \\\n",
    "Read input file and slice into manageable sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MICA.lib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython import parallel\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch an ipython parallel cluster\n",
    "Run this on hpc node to launch a cluster with mpi engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this currently needs to be launched from terminal\n",
    "\n",
    "#import subprocess \n",
    "#We need to launch an ipython parallel cluster\n",
    "\n",
    "#!ipcluster start --n=4\n",
    "#subprocess.Popen(['ipcluster','start','--n=4'])\n",
    "#subprocess.Popen(['ipcluster', 'start', '--engines=MPIEngineSetLauncher', '--log-level', 'DEBUG', '--n=4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ipcluster start --engines=MPIEngineSetLauncher --log-level DEBUG --n=4 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a parallel client so that we can use %%px cell magic\n",
    "# With rc and dview, we can interact between mpi ranks and the thread running this notebook\n",
    "rc = ipp.Client()\n",
    "dview = rc[:]\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#load all necessary libraries onto each rank\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "import numba\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy as sci\n",
    "import numpy as np\n",
    "import anndata\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "import fast_histogram\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from MICA.lib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] Hello, World! I am process 0 of 4 on noderome152.\n",
      "[stdout:1] Hello, World! I am process 1 of 4 on noderome152.\n",
      "[stdout:2] Hello, World! I am process 2 of 4 on noderome152.\n",
      "[stdout:3] Hello, World! I am process 3 of 4 on noderome152.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "sys.stdout.write(\n",
    "    \"Hello, World! I am process %d of %d on %s.\\n\" \n",
    "    % (rank, size, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test this code on the terminal (outside of jupyter)\n",
    "The following cell creates a python script and then we run it via command line on the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#this is a defined elsewhere as a standalone python executable\n",
    "#we are duplicating here to experiment with format for distrubuted computing\n",
    "def prep_dist(input_file, out_name, slice_unit):\n",
    "    \"\"\" Preprocess input file to create sliced matrices.\n",
    "\n",
    "    Reads file into HDF5-format, adds parameters, slices data in file, generates several files with\n",
    "    different combinations of slices.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): path to input text-file\n",
    "        out_name   (str): common rootname of generated output files\n",
    "        slice_unit (int): size of each slice of cell data in input text-file\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    #Read in whole file stored in anndata csr format\n",
    "    adf=utils.read_anndata_file(input_file)\n",
    "    #if adf==None :\n",
    "    #    raise Exception(\"Input file \",input_file,\" not found.\")\n",
    "        \n",
    "    print(\"initial data size=\",adf.shape)\n",
    "\n",
    "    #anndata object can be split easily by subsetting\n",
    "\n",
    "    #patch_file\n",
    "    \"\"\" Prepares the HDF5 file for slicing. Completes the \"temporary\" HDF5-format file.\n",
    "    Reads input file into several data frames. Indexes attributes as row, col and slice.\n",
    "    Indexes columns and rows of data. Converts all data frames into an output HDF5 file\n",
    "    with separate keys for each piece of data.\n",
    "    Args:\n",
    "        df_file       (str): path to HDF5-format file\n",
    "        out_file_name (str): path to complete HDF5-format file\n",
    "    \"\"\"\n",
    "    #patch adds parameters to anndata object\n",
    "    #ceb I'm not sure what value the patch function adds\n",
    "    \n",
    "    # prepare h5 files (whole)\n",
    "    #h5_tmp = out_name + \".h5ad.tmp\"\n",
    "    #utils.patch_anndata_csr_file(h5_tmp, out_name)\n",
    "\n",
    "    #adata_sub = utils.patch_anndata_csr_file(adf, out_name)\n",
    "\n",
    "    #os.remove(h5_tmp)\n",
    "    \n",
    "    #def patch_file(df_file,  out_file_name):\n",
    "    #df = pd.HDFStore(df_file)[\"slice_0\"]\n",
    "    #df.to_hdf(out_file_name + \".whole.h5\", \"slice_0\")\n",
    "    #pd.DataFrame(data=np.array(df.shape + (1,)), index=[\"row\", \"col\", \"slice\"]).to_hdf(\n",
    "    #    out_file_name + \".whole.h5\", \"attr\"\n",
    "    #)\n",
    "    #pd.DataFrame(data=df.columns, index=df.columns).to_hdf(\n",
    "    #    out_file_name + \".whole.h5\", \"cols\"\n",
    "    #)\n",
    "    #pd.DataFrame(data=df.index, index=df.index).to_hdf(\n",
    "    #    out_file_name + \".whole.h5\", \"rows\"\n",
    "    #)    \n",
    "    \n",
    "    \n",
    "    #Slice_file\n",
    "    #def slice_file(df_file,  out_file_name, slice_size=\"1000\"):\n",
    "    \"\"\" Slices the HDF5 file.\n",
    "    Determines the number of slices for the data, based on slice_size.\n",
    "    Calculates start and end indices for slicing, creating a new dataframe\n",
    "    based on those indices and appends them to the sliced output file using\n",
    "    a unique-identifier in the format 'slice_00x' as key.\n",
    "    Args:\n",
    "        df_file       (str): path to HDF5-format file\n",
    "        out_file_name (str): path to sliced output HDF5 file\n",
    "        slice_size    (str): number of items in each slice\n",
    "    \"\"\"\n",
    "    \n",
    "    #h5_whole = out_name + \".whole.h5ad\"\n",
    "    slice_size = int(slice_unit)\n",
    "    \n",
    "    #df = pd.HDFStore(df_file)[\"slice_0\"]\n",
    "    \n",
    "    #compute number of slices needed to break dataset in to slice_size row blocks\n",
    "    b = int(np.ceil(float(adf.shape[0]) / float(slice_size)))\n",
    "    #determine how many digits are in b so we can pad spaces for the string output\n",
    "    digit = int(np.floor(np.log10(b)) + 1)\n",
    "    #loop over slice numbers\n",
    "    for i in range(b):\n",
    "        #slice name is equal to batch index\n",
    "        slice_name = str(i).zfill(digit)\n",
    "        #compute batch row indices\n",
    "        start = i * slice_size\n",
    "        end = np.min([(i + 1) * slice_size, adf.shape[0]])\n",
    "        #copy slice to array of slices\n",
    "        adf_sub=adf[start:end,:]\n",
    "        #write to file so we don't have to keep each slice in memory\n",
    "        output_file_name = out_name + \".slice_\" + slice_name +\".h5ad\"\n",
    "        print(\"output_file_name: \",output_file_name)\n",
    "        adf_sub.write(output_file_name)\n",
    "        \n",
    "    #return nrows and nslices\n",
    "    return adf.shape[0], adf.shape[1], b\n",
    "    \n",
    "    #ceb this function creates pairwise block comparison files for each b(b-1)/2 block comparisons to be done.\n",
    "    #calc_mi function is called to run independently on each of these comparison files.\n",
    "    #We can replace this if we use mpi to spawn ranks and then have each rank perform one or more of these block comparisons.\n",
    "    # The mpi rank will read the two files that it needs, so we can reduce the number of files generated\n",
    "    # We can repurpose the following loop to direct the matrix file to the mpi ranks\n",
    "    \n",
    "\n",
    "  \n",
    "    #in_.close()    \n",
    "    #logging.info('MICA-prep step completed successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#ceb create csr version of numba_histogram2d, also compute_bin with knowledge that minx will always be zero\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_nan_fill(x):\n",
    "    shape = x.shape\n",
    "    x = x.ravel()\n",
    "    x[np.isnan(x)] = 0.0\n",
    "    x = x.reshape(shape)\n",
    "    return x\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def numba_inf_fill(x):\n",
    "    shape = x.shape\n",
    "    x = x.ravel()\n",
    "    x[np.isinf(x)] = 0.0\n",
    "    x = x.reshape(shape)\n",
    "    return x\n",
    "\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def compute_bin_upperbound(x, max, num_bins):\n",
    "    \"\"\" Compute bin index for a give number.\n",
    "        Assume that min is always zero\n",
    "    \"\"\"\n",
    "    # special case to mirror NumPy behavior for last bin\n",
    "    if x == max:\n",
    "        return num_bins - 1 # a_max always in last bin\n",
    "\n",
    "    bin = int(num_bins * x / max)\n",
    "\n",
    "    if bin >= num_bins:\n",
    "        return None\n",
    "    else:\n",
    "        return bin\n",
    "\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def numba_histogram2d_csr(arr1, cols1, arr2, cols2, ncols, num_bins):\n",
    "    \"\"\" Compute the bi-dimensional histogram of two data samples.\n",
    "    Args:\n",
    "        arr1 (array_like, shape (N,)): An array containing the x coordinates of the points to be histogrammed.\n",
    "        arr2 (array_like, shape (N,)): An array containing the y coordinates of the points to be histogrammed.\n",
    "        num_bins (int): int\n",
    "    Return:\n",
    "        hist (2D ndarray)\n",
    "    \"\"\"\n",
    "    #for csr arrays we have to compute zero bins ahead of time \n",
    "        \n",
    "    bin_indices1 = np.zeros((ncols,), dtype=np.int16)\n",
    "    max1 = arr1.max()\n",
    "    #note that bin_indices has same size/indices as full array x and y\n",
    "    for i, x in enumerate(arr1.flat):\n",
    "        #assume zero min\n",
    "        bin_indices1[cols1[i]] = compute_bin_upperbound(x, max1, num_bins)\n",
    "\n",
    "    bin_indices2 = np.zeros((ncols,), dtype=np.int16)\n",
    "    max2 = arr2.max()\n",
    "    for i, y in enumerate(arr2.flat):\n",
    "        #assume zero min\n",
    "        bin_indices2[cols2[i]] = compute_bin_upperbound(y, max2, num_bins)\n",
    "\n",
    "    hist = np.zeros((num_bins, num_bins), dtype=np.int16)\n",
    "    for i, b in enumerate(bin_indices1):\n",
    "        hist[b, bin_indices2[i]] += 1\n",
    "        \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#import numba\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def numba_calc_mi_dis_csr(arr1, cols1, arr2, cols2, bins, m):\n",
    "    \"\"\" Calculates a mutual information distance D(X, Y) = H(X, Y) - I(X, Y) using bin-based method\n",
    "\n",
    "    It takes gene expression data from single cells, and compares them using standard calculation for\n",
    "    mutual information and joint entropy. It builds a 2d histogram, which is used to calculate P(arr1, arr2).\n",
    "\n",
    "    Args:\n",
    "        arr1 (float) nparray of csr: gene expression data for cell 1\n",
    "        cols1 (int): column indices for csr arr1\n",
    "        arr2 (float) nparray of csr: gene expression data for cell 2\n",
    "        cols2 (int): column indices for csr arr\n",
    "        bins           (int): number of bins\n",
    "        m              (int): number of genes\n",
    "    Returns:\n",
    "        a float between 0 and 1\n",
    "    \"\"\"\n",
    "    hist = numba_histogram2d_csr(arr1, cols1, arr2, cols2, m, bins)\n",
    "    \n",
    "    sm = np.sum(hist, axis=1)\n",
    "    tm = np.sum(hist, axis=0)\n",
    "    sm = sm / float(sm.sum())\n",
    "    tm = tm / float(tm.sum())\n",
    "\n",
    "    sm_tm = np.zeros((bins, bins), dtype=np.float32)\n",
    "    for i, s in enumerate(sm):\n",
    "        for j, t in enumerate(tm):\n",
    "            sm_tm[i, j] = s * t\n",
    "\n",
    "    fq = hist / float(m)\n",
    "    div = np.true_divide(fq, sm_tm)\n",
    "    numba_nan_fill(div)\n",
    "    ent = np.log(div)\n",
    "    numba_inf_fill(ent)\n",
    "    agg = np.multiply(fq, ent)\n",
    "    joint_ent = -np.multiply(fq, numba_inf_fill(np.log(fq))).sum()\n",
    "    return joint_ent - agg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#numba compilation cannot interpret the creation of a 2d array inside of this function so we pass in and out SM_block instead of returning it\n",
    "#import numba\n",
    "@numba.jit(nopython=True, fastmath=True)\n",
    "def process_matrices(Arows,Amat_data,Amat_indptr,Amat_indices,\n",
    "                     Brows,Bmat_data,Bmat_indptr,Bmat_indices,\n",
    "                     num_bins,num_genes,\n",
    "                     SM_block, symmetric=False):\n",
    "    #(mat1.n_obs,mat1.X.data,mat1.X.indptr,mat1.X.indices, mat2.n_obs,mat2.X.data,mat2.X.indptr,mat2.X.indices,SM,num_bins,mat2.n_vars,symmetric)\n",
    "\n",
    "    #Arows = mat1.n_obs\n",
    "    #Brows = mat2.n_obs\n",
    "    #num_genes = mat1.n_vars\n",
    "    \n",
    "    #SM_block = np.ndarray(shape=(Arows,Brows))#, dtype=float, order='F')\n",
    "    #SM_block = SM_block.reshape(Arows,Brows)\n",
    "    for i in range(Arows):\n",
    "        Arowstart = Amat_indptr[i]\n",
    "        Arowend   = Amat_indptr[i+1]\n",
    "        Arow_cols = Amat_indices[Arowstart:Arowend]\n",
    "        Arow_data = Amat_data[Arowstart:Arowend]\n",
    "\n",
    "        Bstart=0\n",
    "        Bend=Brows\n",
    "        #if(symmetric):Bstart=i #upper triangluar\n",
    "        if(symmetric):Bend=i+1 #lower triangular\n",
    "        for j in range(Bstart,Bend): \n",
    "            Browstart = Bmat_indptr[j]\n",
    "            Browend   = Bmat_indptr[j+1]\n",
    "            Brow_cols = Bmat_indices[Browstart:Browend]\n",
    "            Brow_data = Bmat_data[Browstart:Browend]               \n",
    "            SM_block[i,j] = numba_calc_mi_dis_csr(Arow_data, Arow_cols, Brow_data, Brow_cols, num_bins, num_genes)\n",
    "            #SM_block[i*Arows+j] = numba_calc_mi_dis_csr(Arow_data, Arow_cols, Brow_data, Brow_cols, num_bins, num_genes)\n",
    "\n",
    "            \n",
    "    return #SM_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#from mpi4py import MPI\n",
    "from scipy.sparse import csr_matrix\n",
    "#import sys\n",
    "import pandas as pd\n",
    "\n",
    "#create a 2d list to hold blocks of similarity matrix\n",
    "#this should be a global var\n",
    "#SM = [[None for j in range(b)] for i in range(b)] \n",
    "\n",
    "def calc_distance_metric_distributed(in_file_path, project_name, nrows, ncols, nslices, SM):\n",
    "    \n",
    "    \"\"\" Prepares the already sliced input file for further calculation in MICA.\n",
    "    Enters pairs of slices (matrices) into temporary HDF5-format files. It enters them\n",
    "    individually, using their unique key. It also enters the parameter data for every single \n",
    "    pair into the key \"params\", which consists of: [key1, key2, num_bins, num_genes,\n",
    "    pair_index, project_name, num_slices]\n",
    "    Args:\n",
    "        in_file      (str): path to sliced HDF5-format file\n",
    "        project_name (str): project name used to generate path for final outputs\n",
    "        nrows : number of rows in global matrix\n",
    "        ncols : number of vars in global matrix\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #create a 2d list to hold blocks of similarity matrix\n",
    "    #this should be a global var\n",
    "    #SM = [[None for j in range(b)] for i in range(b)] \n",
    "    \n",
    "    \n",
    "    #input file is full input that has been segmented into b blocks of rows\n",
    "    \n",
    "    #nranks would ideally be equal to  b(b+1)/2\n",
    "    comm = MPI.COMM_WORLD\n",
    "    size = comm.Get_size()\n",
    "    myrank = comm.Get_rank()\n",
    "    name = MPI.Get_processor_name()\n",
    "    #sys.stdout.write(\"Hello, World! I am process %d of %d on %s.\\n\" % (myrank, size, name))\n",
    "    \n",
    "    #in_ = pd.HDFStore(in_file, \"r\")  # slice.h5\n",
    "    #b = int(np.ceil(float(nrows) / float(slice_size)))\n",
    "    digit = int(np.floor(np.log10(nslices)) + 1)\n",
    "\n",
    "    num_bins = int(np.floor(nrows ** (1 / 3.0)))  # number of bins\n",
    "    #print(\"bins= \",bins)\n",
    "    #b = in_[\"attr\"].loc[\"slice\", 0]  # number of sliced matrix\n",
    "    b = nslices #number of row blocks (cells)\n",
    "    m = ncols  # number of cols per row (genes)\n",
    "\n",
    "    n_block_comparisons = int((b * (b + 1)) / 2)  # total number of row block comparisons needed to compute entire global similarity matrix\n",
    "    n_jobs_per_rank = n_block_comparisons/size\n",
    "    if (myrank == 0): print(\"block comparsons = %d. jobs per rank = %d\\n\" % (n_block_comparisons, n_jobs_per_rank))\n",
    "\n",
    "    #build list of row block comparisons that current mpi rank will process\n",
    "    myslices=[]\n",
    "    for i in range(b):\n",
    "        for j in range(i,b): # j in range [i,b]\n",
    "            idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "            targetrank = idx//n_jobs_per_rank\n",
    "            if (targetrank == myrank): myslices.append((i,j))            \n",
    "\n",
    "    #from list just generated, only do work assigned to your rank\n",
    "    for index, tuple in enumerate(myslices):\n",
    "        #print(\"tuple=\",tuple)\n",
    "        i=tuple[0] #block row\n",
    "        j=tuple[1] #block col      \n",
    "\n",
    "        #get 1st slice (row block) file\n",
    "        slice_name = str(i).zfill(digit)\n",
    "        ##ceb we only want to read this once per i,j combination\n",
    "        input_file = in_file_path + project_name + \".slice_\" + slice_name +\".h5ad\"\n",
    "        #print(\"infile seg1: \",input_file)\n",
    "        mat1 = utils.read_anndata_file(input_file)\n",
    "    \n",
    "\n",
    "        #get 2nd slice (row block) file\n",
    "        slice_name = str(j).zfill(digit)\n",
    "        input_file = in_file_path + project_name + \".slice_\" + slice_name +\".h5ad\"\n",
    "        #print(\"infile seg2: \",input_file)\n",
    "        mat2 = utils.read_anndata_file(input_file)    \n",
    "\n",
    "        #check to see if block comparison will result in a symmetric SM matrix\n",
    "        # so that we can reduce the number of computations in half\n",
    "        symmetric=False\n",
    "        if i==j: symmetric=True\n",
    "        \n",
    "        print(\"rank: \",myrank,\" comparison between segs:\",i,\" x \",j,\" symmetric=\",symmetric)\n",
    "            \n",
    "        #compute distance metrics between row blocks\n",
    "        \n",
    "        Arows = mat1.n_obs\n",
    "        Brows = mat2.n_obs\n",
    "        num_genes = mat1.n_vars #we will assume Acols==Bcols==num_genes\n",
    "        \n",
    "        #need SM for each block pair    \n",
    "        #creates local array of zeros and assigns to global 2d list\n",
    "        #create matrix of zeros with row order indexing\n",
    "        SM[i][j] = np.zeros(shape=(Arows,Brows), dtype = float, order = 'C')\n",
    "        #create a 1d array for easy transfer via mpi\n",
    "        #SM[i][j] = np.zeros(shape=(Arows*Brows), dtype = float, order = 'C')\n",
    "\n",
    " \n",
    "        #This numba function cannot create a numpy array internally so we return SM[i,j] as a variable\n",
    "        process_matrices(mat1.n_obs,mat1.X.data,mat1.X.indptr,mat1.X.indices, \n",
    "                         mat2.n_obs,mat2.X.data,mat2.X.indptr,mat2.X.indices,\n",
    "                         num_bins, num_genes, SM[i][j],\n",
    "                         symmetric #if i==j we can eliminate half of computations\n",
    "                        )\n",
    "\n",
    "        #convert to csr\n",
    "        #we may want to assign this to scalapack distributed matrix here\n",
    "        #SM[i][j]=csr_matrix(SM[i][j])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "import os\n",
    "cwd=os.getcwd()\n",
    "if rank==0:\n",
    "    print(cwd)\n",
    "    \n",
    "data_file_path = cwd+'/test_data/inputs/10x/PBMC/3k/pre-processed/'\n",
    "input_file_name = data_file_path + 'pbmc3k_preprocessed.h5ad'\n",
    "project_name = 'pbmc3k'\n",
    "output_file_name = data_file_path+project_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#set slice size (max size of row blocks)\n",
    "slice_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print (input_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "initial data size= (2496, 10499)\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_0.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_1.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_2.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_3.h5ad\n",
      "output_file_name:  /research/rgs01/home/clusterHome/cburdysh/MICA_Project/MICA_distributed/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k.slice_4.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stderr:0] \n",
      "/home/cburdysh/.conda/envs/py36/lib/python3.6/site-packages/anndata/_core/anndata.py:1094: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if not is_categorical(df_full[k]):\n",
      "/home/cburdysh/.conda/envs/py36/lib/python3.6/site-packages/anndata/_core/anndata.py:1192: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  if is_string_dtype(df[key]) and not is_categorical(df[key])\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#Run prep.py only on one processor to create the slice files\n",
    "\n",
    "g_nrows=0 #global number of rows (cells)\n",
    "ncols=0\n",
    "nslices=0\n",
    "if rank==0: \n",
    "    g_nrows, ncols, nslices = prep_dist(input_file_name, output_file_name, slice_size)\n",
    "\n",
    "# this uses the iparallel dview object to distribute these variables from the notebook thread to all mpi ranks\n",
    "# but the ranks can't communicate back to the notebook thread\n",
    "#dview.push(dict(nrows=nrows, ncols=ncols, nslices=nslices))\n",
    "\n",
    "#broadcast resultant variables from root to the other ranks\n",
    "g_nrows = comm.bcast(g_nrows, root=0)\n",
    "ncols = comm.bcast(ncols, root=0)\n",
    "nslices = comm.bcast(nslices, root=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] global nrows, ncols, slices:  2496 10499 5\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print(\"global nrows, ncols, slices: \",g_nrows, ncols, nslices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in anndata preprocessed files (in distributed mode, by node number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "block comparsons = 15. jobs per rank = 3\n",
      "\n",
      "rank:  0  comparison between segs: 0  x  0  symmetric= True\n",
      "rank:  0  comparison between segs: 0  x  1  symmetric= False\n",
      "rank:  0  comparison between segs: 0  x  2  symmetric= False\n",
      "rank:  0  comparison between segs: 0  x  3  symmetric= False\n",
      "Elapsed = 35.57201290130615\n",
      "[stdout:1] \n",
      "rank:  1  comparison between segs: 0  x  4  symmetric= False\n",
      "rank:  1  comparison between segs: 1  x  1  symmetric= True\n",
      "rank:  1  comparison between segs: 1  x  2  symmetric= False\n",
      "rank:  1  comparison between segs: 1  x  3  symmetric= False\n",
      "Elapsed = 36.69508695602417\n",
      "[stdout:2] \n",
      "rank:  2  comparison between segs: 1  x  4  symmetric= False\n",
      "rank:  2  comparison between segs: 2  x  2  symmetric= True\n",
      "rank:  2  comparison between segs: 2  x  3  symmetric= False\n",
      "rank:  2  comparison between segs: 2  x  4  symmetric= False\n",
      "Elapsed = 36.95906114578247\n",
      "[stdout:3] \n",
      "rank:  3  comparison between segs: 3  x  3  symmetric= True\n",
      "rank:  3  comparison between segs: 3  x  4  symmetric= False\n",
      "rank:  3  comparison between segs: 4  x  4  symmetric= True\n",
      "Elapsed = 22.57027006149292\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#create a 2d list to hold blocks of similarity matrix\n",
    "#this should be stored in a distributed scalapack matrix\n",
    "b=nslices #row blocks\n",
    "SM = [[None for j in range(b)] for i in range(b)] \n",
    "\n",
    "start = time.time()\n",
    "calc_distance_metric_distributed(data_file_path, project_name, g_nrows, ncols, nslices, SM)\n",
    "end = time.time()\n",
    "print(\"Elapsed = %s\" % (end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "(500, 500)\n",
      "[[-5.15595322e-09  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.57519216e-01  1.31353685e-08  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.16255376e-01  7.22905476e-01 -1.01818249e-08 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 6.00543187e-01  6.95598780e-01  6.67865236e-01 ...  1.78994189e-08\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 5.21585065e-01  6.27581208e-01  5.83600189e-01 ...  5.78146991e-01\n",
      "  -1.53148856e-08  0.00000000e+00]\n",
      " [ 5.47296983e-01  6.46476002e-01  6.09194393e-01 ...  6.03402223e-01\n",
      "   5.24108924e-01  1.59023507e-08]]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#Note diagonal is small. Should we expect diagonal to be unity since MI of X,X^T should be 1?\n",
    "if rank==0:\n",
    "    print( np.shape(SM[0][0]) )\n",
    "    print( (SM[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create distributed matrix for scalapack and copy distributed blocks into object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Rank: 0  SM[ 0 ][ 0 ] size= (500, 500)\n",
      "Rank: 0  SM[ 0 ][ 1 ] size= (500, 500)\n",
      "Rank: 0  SM[ 0 ][ 2 ] size= (500, 500)\n",
      "Rank: 0  SM[ 0 ][ 3 ] size= (500, 500)\n",
      "[stdout:1] \n",
      "Rank: 1  SM[ 0 ][ 4 ] size= (500, 496)\n",
      "Rank: 1  SM[ 1 ][ 1 ] size= (500, 500)\n",
      "Rank: 1  SM[ 1 ][ 2 ] size= (500, 500)\n",
      "Rank: 1  SM[ 1 ][ 3 ] size= (500, 500)\n",
      "[stdout:2] \n",
      "Rank: 2  SM[ 1 ][ 4 ] size= (500, 496)\n",
      "Rank: 2  SM[ 2 ][ 2 ] size= (500, 500)\n",
      "Rank: 2  SM[ 2 ][ 3 ] size= (500, 500)\n",
      "Rank: 2  SM[ 2 ][ 4 ] size= (500, 496)\n",
      "[stdout:3] \n",
      "Rank: 3  SM[ 3 ][ 3 ] size= (500, 500)\n",
      "Rank: 3  SM[ 3 ][ 4 ] size= (500, 496)\n",
      "Rank: 3  SM[ 4 ][ 4 ] size= (496, 496)\n"
     ]
    }
   ],
   "source": [
    "%%px \n",
    "from scipy.sparse import csr_matrix #may not use csr as it complicates copy to distributed scalapack and is not used in scalapack apparently\n",
    "import collections\n",
    "for i in range(b):\n",
    "    for j in range(i,b): # j in range [i,b]\n",
    "        if isinstance(SM[i][j], collections.Iterable):\n",
    "            #lm=SM[i][j]#.ravel()\n",
    "            #print(SM[i][j])\n",
    "            #B=csr_matrix(SM[0][0]) \n",
    "            #print(\"Rank:\",rank, \" SM[\",i,\"][\",j,\"] nnz=\",SM[i][j].getnnz)\n",
    "            #print(\"Rank:\",rank, \" SM[\",i,\"][\",j,\"] nnz=\",SM[i][j])\n",
    "            print(\"Rank:\",rank, \" SM[\",i,\"][\",j,\"] size=\",np.shape(SM[i][j]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate a global array with all of the MI data from each rank\n",
    "\n",
    "Preferably, we would like each rank to contribute of their block MI matrices to the global matrix,\n",
    "but currently the distributed global matrix has to be constructed from a global (not distributed) array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, each rank loads it's matrix contribution into a global block matrix, \\\n",
    "And then we use MPI to transfer these contributions to rank 0 from all other ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We may be able to have each rank copy their data directly to the distributed global matrix (that we have yet to initialize) \n",
    "#so that we can avoid having to agglomerate matrix data on a single rank\n",
    "#dA = dA.np2self(local_a, srow, scol, rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#copy SM[i][j] from each rank to root rank 0\n",
    "n_jobs_per_rank=(int((b * (b + 1)) / 2))/comm.Get_size()\n",
    "import collections\n",
    "for i in range(b):\n",
    "    for j in range(i,b): # j in range [i,b]\n",
    "        #if SM[i][j]!=None:\n",
    "        #if SM[i][j]!=0:\n",
    "        idx = int(i * b + j - (i * (i + 1)) / 2)\n",
    "        srank = idx//n_jobs_per_rank\n",
    "        if rank!=0 and isinstance(SM[i][j], collections.Iterable):\n",
    "            comm.send(SM[i][j],dest=0,tag=idx)\n",
    "        elif rank==0 and not isinstance(SM[i][j], collections.Iterable):\n",
    "            SM[i][j]=comm.recv( source=srank, tag=idx)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "(496, 496)\n",
      "[[-1.08149925e-08  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.18987683e-01 -1.94897120e-08  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.56650333e-01  4.91898546e-01 -2.23886729e-09 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 5.30388228e-01  5.69486206e-01  3.70204082e-01 ...  1.50523602e-08\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.89730330e-01  5.22489321e-01  3.17985325e-01 ...  3.97571517e-01\n",
      "  -2.86551806e-08  0.00000000e+00]\n",
      " [ 5.45617907e-01  5.72776800e-01  4.09006844e-01 ...  4.85972394e-01\n",
      "   4.41741719e-01  2.10293315e-08]]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "#Check to see of data was transferred to rank 0\n",
    "if rank==0:\n",
    "    print(np.shape(SM[4][4]))\n",
    "    print(SM[4][4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#For examples: https://www.programmersought.com/article/9237705297/\n",
    "#git clone https://github.com/jrs65/scalapy.git\n",
    "#import scalapy\n",
    "#from scalapy import blacs\n",
    "from scalapy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#from inspect import getmembers, isfunction, ismodule\n",
    "#if rank == 0:\n",
    "#    print([o[0] for o in getmembers(scalapy) if ismodule(o[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%px\n",
    "#from inspect import getmembers, isfunction, ismodule\n",
    "#import scalapy\n",
    "#if rank==0:\n",
    "#    print(getmembers(scalapy.blacs, isfunction))\n",
    "#    #print(getmembers(scalapy.routines, isfunction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#total number of global blocks = total number of block comparisons\n",
    "#Should be greater than number of ranks to improve load balancing\n",
    "# b is number of slices (blocks of rows) original data has been discretized into.\n",
    "#The size of these blocks can be variable\n",
    "global_number_of_matrix_blocks= int((b * (b + 1)) / 2) \n",
    "\n",
    "#We'll use a 2d process grid to distribute blocks so we want to have num_ranks divisivle by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bcast SM[i][j] from each rank to root rank 0 so that we can load global matrix array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert array into distributed block cyclic global matrix\n",
    "Simply load a numpy vector with data from each block matrix to use in scalapak distributed matrix initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import collections\n",
    "\n",
    "#create global 2d array to store MI info which will be sent to create block cyclic distributed matrix\n",
    "#I would like to know how to create this matrix without having to have the memory of a global array on each rank\n",
    "global_MI=np.zeros([g_nrows,g_nrows])\n",
    "\n",
    "blocksize=slice_size\n",
    "if rank==0:\n",
    "    #load rank data into global matrix\n",
    "    for i in range(b):\n",
    "        for j in range(i,b): # j in range [i,b]\n",
    "            #if SM[i][j]!=None:\n",
    "            if isinstance(SM[i][j], collections.Iterable):\n",
    "                lA = SM[i][j]\n",
    "                #print( \"rank=\",rank,\"SM[\",i,\",\",j,\" shape=\",np.shape(lA) )\n",
    "                lr=np.shape(lA)[0]\n",
    "                lc=np.shape(lA)[1]\n",
    "                for ii in range(lr):\n",
    "                    for jj in range(lc):\n",
    "                        global_MI[i*blocksize+ii, j*blocksize+jj] = lA[ii,jj]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "-5.155953219926346e-09\n",
      "2.1029331498390036e-08\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "if rank==0:\n",
    "    print(global_MI[0,0])\n",
    "    print(global_MI[2495,2495])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "PR= 2 PC= 2\n",
      "rank 0 has global_shape of dMI = (2496, 2496)\n",
      "rank 0 has local_shape of dMI = (1280, 1280)\n",
      "rank 0 has block_shape of dMI = (64, 64)\n",
      "[stdout:1] \n",
      "rank 1 has global_shape of dMI = (2496, 2496)\n",
      "rank 1 has local_shape of dMI = (1280, 1216)\n",
      "rank 1 has block_shape of dMI = (64, 64)\n",
      "[stdout:2] \n",
      "rank 2 has global_shape of dMI = (2496, 2496)\n",
      "rank 2 has local_shape of dMI = (1216, 1280)\n",
      "rank 2 has block_shape of dMI = (64, 64)\n",
      "[stdout:3] \n",
      "rank 3 has global_shape of dMI = (2496, 2496)\n",
      "rank 3 has local_shape of dMI = (1216, 1216)\n",
      "rank 3 has block_shape of dMI = (64, 64)\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "from scalapy import blacs\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from mpi4py import MPI\n",
    "from scalapy import core\n",
    "import scalapy.routines as rt\n",
    "\n",
    "#distribute MI components to ranks as scalapack distributed matrix\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size #total number of ranks\n",
    "\n",
    "global_num_rows =g_nrows\n",
    "global_num_cols =g_nrows\n",
    "local_num_rows =g_nrows/b\n",
    "\n",
    "block_size=64 #default is 32\n",
    "#Define process grid with process rows and process cols\n",
    "#We'll use a 2d process grid to distribute blocks so we want to have num_ranks divisible by 2\n",
    "assert((size % 2)==0)\n",
    "#ideally we would like BR and BC to the square root of the num_ranks to get a square process matrix\n",
    "PR=int(np.sqrt(size))\n",
    "PC=PR\n",
    "#if we can't create a square matrix, get next best dimensions\n",
    "if PR*PR!=size:\n",
    "    PC=size//PR\n",
    "if rank==0:\n",
    "    print(\"PR=\",PR, \"PC=\",PC)\n",
    "\n",
    "#create mpi comm context to send to blacs communicator    \n",
    "#process context\n",
    "cntxt = blacs.sys2blacs_handle(comm)\n",
    "\n",
    "#initialize a process grid to distribute matrix blocks to.\n",
    "#blacs.gridinit(ctxt,order='R',BR,BC)\n",
    "\n",
    "#blacs.gridinit(pc,order='R',BR,BC)\n",
    "#core.initmpi([PR, PC], block_shape=[local_num_rows,local_num_rows])\n",
    "core.initmpi([PR, PC],block_shape=[block_size,block_size])\n",
    "\n",
    "#instead of creating a global array on every processor, we want to load a \n",
    "#global array with local data\n",
    "#MI=core.DistributedMatrix(global_shape=[global_num_rows,global_num_cols])\n",
    "#global_MI=np.zeros([g_nrows,g_nrows])\n",
    "#dMI=core.DistributedMatrix.from_global_array(global_MI,rank=0)\n",
    "\n",
    "#convert to fortran array indexing to match scalapack functions\n",
    "global_MI=np.asfortranarray(global_MI)\n",
    "dMI=core.DistributedMatrix.from_global_array(global_MI)\n",
    "\n",
    "\n",
    "print ('rank %d has global_shape of dMI = %s' % (rank, dMI.global_shape))\n",
    "print ('rank %d has local_shape of dMI = %s' % (rank, dMI.local_shape))\n",
    "print ('rank %d has block_shape of dMI = %s' % (rank, dMI.block_shape))\n",
    "\n",
    "\n",
    "#blacs.gridinit(ctxt, b, b)\n",
    "#ranklist = [(0, 0), \n",
    "#            (0, 1), (1, 1),\n",
    "#            (0, 2), (1, 2), (2, 2),\n",
    "#            (0, 3), (1, 3), (2, 3), (3, 3),\n",
    "#            (0, 4), (1, 4), (2, 4), (3, 4), (4, 4)\n",
    "#           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to preserve this result by writing to file using MPI_IO\n",
    "#mpi_writematrix(fname, local_array, comm, gshape, dtype,\n",
    "#                    blocksize, process_grid, order='F', displacement=0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Frobenius norm of distributed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] ['WorkArray', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_call_routine', '_doc_pblas', '_doc_redist', '_doc_scl', '_encode_strings', '_expand_dm', '_expand_work', '_mod_dict', '_pblas', '_redist', '_scl', '_wrap_routine', 'bdlaexc', 'bdtrexc', 'bslaexc', 'bstrexc', 'cdbtf2', 'cdbtrf', 'cdttrf', 'cdttrsv', 'clahqr2', 'clamsh', 'clanv2', 'claref', 'core', 'cpttrsv', 'csteqr2', 'ddbtf2', 'ddbtrf', 'ddttrf', 'ddttrsv', 'division', 'dlamsh', 'dlapst', 'dlar1va', 'dlaref', 'dlarrd2', 'dlarre2', 'dlarre2a', 'dlarrf2', 'dlarrv2', 'dlasorte', 'dlasrt2', 'dpttrsv', 'dstegr2', 'dstegr2a', 'dstein2', 'dsteqr2', 'expand_args', 'np', 'pblas', 'pcagemv', 'pcahemv', 'pcamax', 'pcatrmv', 'pcaxpy', 'pccopy', 'pcdbsv', 'pcdbtrf', 'pcdbtrs', 'pcdbtrsv', 'pcdotc', 'pcdotu', 'pcdtsv', 'pcdttrf', 'pcdttrs', 'pcdttrsv', 'pcgbsv', 'pcgbtrf', 'pcgbtrs', 'pcgeadd', 'pcgebd2', 'pcgebrd', 'pcgecon', 'pcgeequ', 'pcgehd2', 'pcgehrd', 'pcgelq2', 'pcgelqf', 'pcgels', 'pcgemm', 'pcgemr2d', 'pcgemv', 'pcgeql2', 'pcgeqlf', 'pcgeqpf', 'pcgeqr2', 'pcgeqrf', 'pcgerc', 'pcgerfs', 'pcgerq2', 'pcgerqf', 'pcgeru', 'pcgesv', 'pcgesvd', 'pcgesvx', 'pcgetf2', 'pcgetrf', 'pcgetri', 'pcgetrs', 'pcggqrf', 'pcggrqf', 'pcheev', 'pcheevd', 'pcheevr', 'pcheevx', 'pchegs2', 'pchegst', 'pchegvx', 'pchemm', 'pchemv', 'pchengst', 'pchentrd', 'pcher', 'pcher2', 'pcher2k', 'pcherk', 'pchetd2', 'pchetrd', 'pclabrd', 'pclacgv', 'pclacon', 'pclaconsb', 'pclacp2', 'pclacp3', 'pclacpy', 'pclahqr', 'pclahrd', 'pclamr1d', 'pclapiv', 'pclapv2', 'pclaqge', 'pclaqsy', 'pclarf', 'pclarfb', 'pclarfc', 'pclarfg', 'pclarft', 'pclarz', 'pclarzb', 'pclarzc', 'pclarzt', 'pclascl', 'pclase2', 'pclaset', 'pclasmsub', 'pclassq', 'pclaswp', 'pclatrd', 'pclatrz', 'pclattrs', 'pclauu2', 'pclauum', 'pclawil', 'pcmax1', 'pcpbsv', 'pcpbtrf', 'pcpbtrs', 'pcpbtrsv', 'pcpocon', 'pcpoequ', 'pcporfs', 'pcposv', 'pcposvx', 'pcpotf2', 'pcpotrf', 'pcpotri', 'pcpotrs', 'pcptsv', 'pcpttrf', 'pcpttrs', 'pcpttrsv', 'pcscal', 'pcsscal', 'pcstein', 'pcswap', 'pcsymm', 'pcsyr2k', 'pcsyrk', 'pctradd', 'pctranc', 'pctranu', 'pctrevc', 'pctrmm', 'pctrmr2d', 'pctrmv', 'pctrsm', 'pctrsv', 'pctrti2', 'pctrtri', 'pctrtrs', 'pctzrzf', 'pcung2l', 'pcung2r', 'pcungl2', 'pcunglq', 'pcungql', 'pcungqr', 'pcungr2', 'pcungrq', 'pcunm2l', 'pcunm2r', 'pcunmbr', 'pcunmhr', 'pcunml2', 'pcunmlq', 'pcunmql', 'pcunmqr', 'pcunmr2', 'pcunmr3', 'pcunmrq', 'pcunmrz', 'pcunmtr', 'pdagemv', 'pdamax', 'pdasum', 'pdasymv', 'pdatrmv', 'pdaxpy', 'pdcopy', 'pddbsv', 'pddbtrf', 'pddbtrs', 'pddbtrsv', 'pddot', 'pddtsv', 'pddttrf', 'pddttrs', 'pddttrsv', 'pdgbsv', 'pdgbtrf', 'pdgbtrs', 'pdgeadd', 'pdgebal', 'pdgebd2', 'pdgebrd', 'pdgecon', 'pdgeequ', 'pdgehd2', 'pdgehrd', 'pdgelq2', 'pdgelqf', 'pdgels', 'pdgemm', 'pdgemr2d', 'pdgemv', 'pdgeql2', 'pdgeqlf', 'pdgeqpf', 'pdgeqr2', 'pdgeqrf', 'pdger', 'pdgerfs', 'pdgerq2', 'pdgerqf', 'pdgesv', 'pdgesvd', 'pdgesvx', 'pdgetf2', 'pdgetrf', 'pdgetri', 'pdgetrs', 'pdggqrf', 'pdggrqf', 'pdlabad', 'pdlabrd', 'pdlacon', 'pdlaconsb', 'pdlacp2', 'pdlacp3', 'pdlacpy', 'pdlaed0', 'pdlaed1', 'pdlaed2', 'pdlaed3', 'pdlahqr', 'pdlahrd', 'pdlamr1d', 'pdlamve', 'pdlapiv', 'pdlapv2', 'pdlaqge', 'pdlaqr1', 'pdlaqr2', 'pdlaqr3', 'pdlaqr4', 'pdlaqsy', 'pdlarf', 'pdlarfb', 'pdlarfg', 'pdlarft', 'pdlarz', 'pdlarzb', 'pdlarzt', 'pdlascl', 'pdlase2', 'pdlaset', 'pdlasmsub', 'pdlasrt', 'pdlassq', 'pdlaswp', 'pdlatrd', 'pdlatrz', 'pdlauu2', 'pdlauum', 'pdlawil', 'pdnrm2', 'pdorg2l', 'pdorg2r', 'pdorgl2', 'pdorglq', 'pdorgql', 'pdorgqr', 'pdorgr2', 'pdorgrq', 'pdorm2l', 'pdorm2r', 'pdormbr', 'pdormhr', 'pdorml2', 'pdormlq', 'pdormql', 'pdormqr', 'pdormr2', 'pdormr3', 'pdormrq', 'pdormrz', 'pdormtr', 'pdpbsv', 'pdpbtrf', 'pdpbtrs', 'pdpbtrsv', 'pdpocon', 'pdpoequ', 'pdporfs', 'pdposv', 'pdposvx', 'pdpotf2', 'pdpotrf', 'pdpotri', 'pdpotrs', 'pdptsv', 'pdpttrf', 'pdpttrs', 'pdpttrsv', 'pdrot', 'pdscal', 'pdstebz', 'pdstedc', 'pdstein', 'pdswap', 'pdsyev', 'pdsyevd', 'pdsyevr', 'pdsyevx', 'pdsygs2', 'pdsygst', 'pdsygvx', 'pdsymm', 'pdsymv', 'pdsyngst', 'pdsyntrd', 'pdsyr', 'pdsyr2', 'pdsyr2k', 'pdsyrk', 'pdsytd2', 'pdsytrd', 'pdtradd', 'pdtran', 'pdtrmm', 'pdtrmr2d', 'pdtrmv', 'pdtrord', 'pdtrsen', 'pdtrsm', 'pdtrsv', 'pdtrti2', 'pdtrtri', 'pdtrtrs', 'pdtzrzf', 'pdzasum', 'pdznrm2', 'picopy', 'pigemr2d', 'pitrmr2d', 'print_function', 'psagemv', 'psamax', 'psasum', 'psasymv', 'psatrmv', 'psaxpy', 'pscasum', 'pscnrm2', 'pscopy', 'psdbsv', 'psdbtrf', 'psdbtrs', 'psdbtrsv', 'psdot', 'psdtsv', 'psdttrf', 'psdttrs', 'psdttrsv', 'psgbsv', 'psgbtrf', 'psgbtrs', 'psgeadd', 'psgebal', 'psgebd2', 'psgebrd', 'psgecon', 'psgeequ', 'psgehd2', 'psgehrd', 'psgelq2', 'psgelqf', 'psgels', 'psgemm', 'psgemr2d', 'psgemv', 'psgeql2', 'psgeqlf', 'psgeqpf', 'psgeqr2', 'psgeqrf', 'psger', 'psgerfs', 'psgerq2', 'psgerqf', 'psgesv', 'psgesvd', 'psgesvx', 'psgetf2', 'psgetrf', 'psgetri', 'psgetrs', 'psggqrf', 'psggrqf', 'pslabad', 'pslabrd', 'pslacon', 'pslaconsb', 'pslacp2', 'pslacp3', 'pslacpy', 'pslaed0', 'pslaed1', 'pslaed2', 'pslaed3', 'pslahqr', 'pslahrd', 'pslamr1d', 'pslamve', 'pslapiv', 'pslapv2', 'pslaqge', 'pslaqr1', 'pslaqr2', 'pslaqr3', 'pslaqr4', 'pslaqsy', 'pslarf', 'pslarfb', 'pslarfg', 'pslarft', 'pslarz', 'pslarzb', 'pslarzt', 'pslascl', 'pslase2', 'pslaset', 'pslasmsub', 'pslasrt', 'pslassq', 'pslaswp', 'pslatrd', 'pslatrz', 'pslauu2', 'pslauum', 'pslawil', 'psnrm2', 'psorg2l', 'psorg2r', 'psorgl2', 'psorglq', 'psorgql', 'psorgqr', 'psorgr2', 'psorgrq', 'psorm2l', 'psorm2r', 'psormbr', 'psormhr', 'psorml2', 'psormlq', 'psormql', 'psormqr', 'psormr2', 'psormr3', 'psormrq', 'psormrz', 'psormtr', 'pspbsv', 'pspbtrf', 'pspbtrs', 'pspbtrsv', 'pspocon', 'pspoequ', 'psporfs', 'psposv', 'psposvx', 'pspotf2', 'pspotrf', 'pspotri', 'pspotrs', 'psptsv', 'pspttrf', 'pspttrs', 'pspttrsv', 'psrot', 'psscal', 'psstebz', 'psstedc', 'psstein', 'psswap', 'pssyev', 'pssyevd', 'pssyevr', 'pssyevx', 'pssygs2', 'pssygst', 'pssygvx', 'pssymm', 'pssymv', 'pssyngst', 'pssyntrd', 'pssyr', 'pssyr2', 'pssyr2k', 'pssyrk', 'pssytd2', 'pssytrd', 'pstradd', 'pstran', 'pstrmm', 'pstrmr2d', 'pstrmv', 'pstrord', 'pstrsen', 'pstrsm', 'pstrsv', 'pstrti2', 'pstrtri', 'pstrtrs', 'pstzrzf', 'pzagemv', 'pzahemv', 'pzamax', 'pzatrmv', 'pzaxpy', 'pzcopy', 'pzdbsv', 'pzdbtrf', 'pzdbtrs', 'pzdbtrsv', 'pzdotc', 'pzdotu', 'pzdscal', 'pzdtsv', 'pzdttrf', 'pzdttrs', 'pzdttrsv', 'pzgbsv', 'pzgbtrf', 'pzgbtrs', 'pzgeadd', 'pzgebd2', 'pzgebrd', 'pzgecon', 'pzgeequ', 'pzgehd2', 'pzgehrd', 'pzgelq2', 'pzgelqf', 'pzgels', 'pzgemm', 'pzgemr2d', 'pzgemv', 'pzgeql2', 'pzgeqlf', 'pzgeqpf', 'pzgeqr2', 'pzgeqrf', 'pzgerc', 'pzgerfs', 'pzgerq2', 'pzgerqf', 'pzgeru', 'pzgesv', 'pzgesvd', 'pzgesvx', 'pzgetf2', 'pzgetrf', 'pzgetri', 'pzgetrs', 'pzggqrf', 'pzggrqf', 'pzheev', 'pzheevd', 'pzheevr', 'pzheevx', 'pzhegs2', 'pzhegst', 'pzhegvx', 'pzhemm', 'pzhemv', 'pzhengst', 'pzhentrd', 'pzher', 'pzher2', 'pzher2k', 'pzherk', 'pzhetd2', 'pzhetrd', 'pzlabrd', 'pzlacgv', 'pzlacon', 'pzlaconsb', 'pzlacp2', 'pzlacp3', 'pzlacpy', 'pzlahqr', 'pzlahrd', 'pzlamr1d', 'pzlapiv', 'pzlapv2', 'pzlaqge', 'pzlaqsy', 'pzlarf', 'pzlarfb', 'pzlarfc', 'pzlarfg', 'pzlarft', 'pzlarz', 'pzlarzb', 'pzlarzc', 'pzlarzt', 'pzlascl', 'pzlase2', 'pzlaset', 'pzlasmsub', 'pzlassq', 'pzlaswp', 'pzlatrd', 'pzlatrz', 'pzlattrs', 'pzlauu2', 'pzlauum', 'pzlawil', 'pzmax1', 'pzpbsv', 'pzpbtrf', 'pzpbtrs', 'pzpbtrsv', 'pzpocon', 'pzpoequ', 'pzporfs', 'pzposv', 'pzposvx', 'pzpotf2', 'pzpotrf', 'pzpotri', 'pzpotrs', 'pzptsv', 'pzpttrf', 'pzpttrs', 'pzpttrsv', 'pzscal', 'pzstein', 'pzswap', 'pzsymm', 'pzsyr2k', 'pzsyrk', 'pztradd', 'pztranc', 'pztranu', 'pztrevc', 'pztrmm', 'pztrmr2d', 'pztrmv', 'pztrsm', 'pztrsv', 'pztrti2', 'pztrtri', 'pztrtrs', 'pztzrzf', 'pzung2l', 'pzung2r', 'pzungl2', 'pzunglq', 'pzungql', 'pzungqr', 'pzungr2', 'pzungrq', 'pzunm2l', 'pzunm2r', 'pzunmbr', 'pzunmhr', 'pzunml2', 'pzunmlq', 'pzunmql', 'pzunmqr', 'pzunmr2', 'pzunmr3', 'pzunmrq', 'pzunmrz', 'pzunmtr', 'redist', 'rname', 'robj', 'scalapack', 'sdbtf2', 'sdbtrf', 'sdttrf', 'sdttrsv', 'slamsh', 'slapst', 'slar1va', 'slaref', 'slarrd2', 'slarre2', 'slarre2a', 'slarrf2', 'slarrv2', 'slasorte', 'slasrt2', 'spttrsv', 'sstegr2', 'sstegr2a', 'sstein2', 'ssteqr2', 'util', 'zdbtf2', 'zdbtrf', 'zdttrf', 'zdttrsv', 'zlahqr2', 'zlamsh', 'zlanv2', 'zlaref', 'zpttrsv', 'zsteqr2']\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "from scalapy import lowlevel as ll\n",
    "#norm mutualinfomation matrix\n",
    "\n",
    "#\"\"\"Normalizes mutual information metric in the merged matrix\n",
    "#    \n",
    "#    Args:\n",
    "#        in_mat_file   (str): path to merged matrix\n",
    "#        out_file_name (str): name of output file\n",
    "#    \"\"\"\n",
    "#    #hdf = pd.HDFStore(in_mat_file)\n",
    "#    #df = hdf[\"mi\"]\n",
    "#    #diag = np.asmatrix(np.diag(df))\n",
    "#    #if in_mat_file == out_file_name + \"_dist.h5\":\n",
    "#    #    hdf.put(\"norm_mi\", df / np.sqrt(np.matmul(diag.T, diag)))\n",
    "#        \n",
    "#    #get diag\n",
    "    \n",
    "#    #compute dot product of diag\n",
    "#A2 = scalapy.dot(D, D, transA='T')    \n",
    "    #divide MI matrix by dot product of diag\n",
    "    \n",
    "#Some functions like plansy are fortran functions and not subroutines. Currently only subroutines are parsed by the scalapy conversion scripts\n",
    "#So we might have to compute things like Norm manually until we create a new parser to handle conversion of fortran functions\n",
    "\n",
    "#PDLANGE( 'F', M, N, A, IA, JA, DESCA, WORK ) #compute frobenius norm\n",
    "#PCLANSY( 'F', UPLO, N, A, IA, JA,DESCA, WORK ) #compute frobenius norm on triangular matrix\n",
    "#info = ll.pslansy\n",
    "if rank==0:\n",
    "    print(dir(ll))\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute eigenvalues,vectors\n",
    "#evals1, evecs1 = scalapy.eigh(dMI, overwrite_a=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "##test blacs\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "from scalapy import blacs\n",
    "ctxt = blacs.sys2blacs_handle(comm)\n",
    "blacs.gridinit(ctxt, 2, 2)\n",
    "ranklist = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "gi = blacs.gridinfo(ctxt)\n",
    "gshape = gi[:2]\n",
    "gpos = gi[2:]\n",
    "assert gshape == (2, 2)\n",
    "assert gpos == ranklist[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "##test scalapay on distributed matrix\n",
    " \n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from mpi4py import MPI\n",
    "from scalapy import core\n",
    "import scalapy.routines as rt\n",
    " \n",
    " \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    " \n",
    "if size != 4:\n",
    "    raise Exception(\"Must run with 4 processes\")\n",
    " \n",
    "# define a function to compare whether two arrays are equal\n",
    "allclose = lambda a, b: np.allclose(a, b, rtol=1e-4, atol=1e-6)\n",
    " \n",
    "# initialize a global ProcessContext object,\n",
    "# which includes the initialization of a 2 x 2 process grid\n",
    "core.initmpi([2, 2], block_shape=[16, 16])\n",
    "\n",
    " \n",
    "N = 300\n",
    "# create a N x N numpy array with random numbers\n",
    "gA = np.random.standard_normal((N, N)).astype(np.float64)\n",
    "gA = np.asfortranarray(gA)\n",
    "# create a DistributedMatrix from gA\n",
    "dA = core.DistributedMatrix.from_global_array(gA, rank=0)\n",
    "print ('rank %d has global_shape of dA = %s' % (rank, dA.global_shape))\n",
    "print ('rank %d has local_shape of dA = %s' % (rank, dA.local_shape))\n",
    "print ('rank %d has block_shape of dA = %s' % (rank, dA.block_shape))\n",
    " \n",
    "# compute the inverse of dA\n",
    "invA, ipiv = rt.inv(dA)\n",
    "# convert to a global numpy array hold by rank 0 only\n",
    "ginvA = invA.to_global_array(rank=0)\n",
    " \n",
    "if rank == 0:\n",
    "    # compare the result with that of scipy.linalg.inv\n",
    "    print ('result equals that of scipy: ', allclose(ginvA, la.inv(gA)))\n",
    " \n",
    "# write dA to file\n",
    "file_name = 'dA.dat'\n",
    "dA.to_file(file_name)\n",
    "# now read it from file and check it equals the original DistributedMatrix\n",
    "dA1 = core.DistributedMatrix.from_file(file_name, dA.global_shape, dA.dtype, dA.block_shape, dA.context)\n",
    "print ('rank %d has dA.local_array == dA1.local_array: %s' % (rank, allclose(dA.local_array, dA1.local_array)))\n",
    " \n",
    "# remove the file\n",
    "if rank == 0:\n",
    "    os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write distributed matrix to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "from scipy.sparse import csr_matrix\n",
    "if comm.Get_rank() == 0:\n",
    "    SM_csr=csr_matrix(SM[0][0],(1000,1000),shape=(1000, 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#show that diagonal blocks are symmetric\n",
    "if comm.Get_rank() == 0:\n",
    "    i=1\n",
    "    j=100\n",
    "    print((SM[0][0])[i,j])\n",
    "    print((SM[0][0])[j,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata_sub1=adata[0:1000,:]\n",
    "#adata_sub2=adata[1000:2000,:]\n",
    "#adata_sub3=adata[2000:,:]\n",
    "\n",
    "#print(adata_sub1.shape)\n",
    "#print(adata_sub2.shape)\n",
    "#print(adata_sub3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can split the anndata object by rows in the following way:\n",
    "#print(adata.X.indptr)\n",
    "#print(adata_sub1.X.indptr)\n",
    "#print(adata_sub2.X.indptr)\n",
    "#print(adata_sub3.X.indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to dataframe for comparison\n",
    "aframe = adata.to_df()\n",
    "bframe = bdata.to_df()\n",
    "aframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(frame.iloc[2000,:])\n",
    "#(frame == 0).astype(int).sum(axis=1)\n",
    "#print(frame[frame == 0].count(axis=1)/len(frame.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#frame.to_csv('../../test_data/outputs/kgraph/pmbc3k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance_mat_dist(mat1, mat2, paras, method):\n",
    "    \"\"\" Calculates a distance metric in between two matrices (slices)\n",
    "\n",
    "    Calculates a distance metric using the preferred method of comparison. Iterates over each cell's\n",
    "    gene expression data and populates a new matrix with rows and columns as cells from the input\n",
    "    matrices. The resulting matrix is then converted to an HDF5-format file.\n",
    "\n",
    "    Args:\n",
    "        mat1  (anndata dataframe): a sliced part of the original matrix, with some fraction of the\n",
    "                                  total cells as rows from original file and all gene expression\n",
    "                                  attributes as columns\n",
    "        mat2  (anndata dataframe): similar to mat1\n",
    "        paras (anndata dataframe): a dataframe that holds an array of parameters from the whole dataset\n",
    "        method             (str): the method to be used for the distance calculation (\n",
    "                                        mutual information: \"mi\")\n",
    "    \"\"\"\n",
    "\n",
    "    bins = int(paras.loc[\"num_bins\", 0])\n",
    "    m = int(paras.loc[\"n_genes\", 0])\n",
    "    key = paras.loc[\"MI_indx\", 0]\n",
    "\n",
    "    project_name = paras.loc[\"project_name\", 0]\n",
    "    out_file_name = project_name + \"_\" + key + \".h5\"\n",
    "    print(out_file_name)\n",
    "\n",
    "    if method == \"mi\":\n",
    "\n",
    "        df = pd.DataFrame(data=0, index=mat1.index, columns=mat2.index) \n",
    "        start = time.time()\n",
    "        \n",
    "        for c in mat2.index:\n",
    "            df.loc[mat1.index, c] = mat1.apply(\n",
    "                calc_mi, axis=1, args=(mat2.loc[c, :], bins, m)\n",
    "            )\n",
    "            \n",
    "        end = time.time()\n",
    "\n",
    "    else:\n",
    "        sys.exit(\"Distance Metrics not supported!\\n\")\n",
    "\n",
    "        \n",
    "    df.to_hdf(out_file_name, str(key))  # key: MI_indx\n",
    "    paras.to_hdf(out_file_name, \"params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #tmp hardcode input file\n",
    "    inputfileA = '../../test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad'\n",
    "    inputfileB = '../../test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad'\n",
    "\n",
    "    #For given rank id, read in A and B matrix slices \n",
    "    matA = anndata.read_h5ad(inputfileA)\n",
    "    matB = anndata.read_h5ad(inputfileB)\n",
    "    metrics = metric.lower()\n",
    "    params = matA[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reproducing standalone calc_scatter function here to experiment with distributed processing\n",
    "def calc_scatter(input_file, metric='mi'):\n",
    "    \"\"\" Calls calc_distance_mat utility function and calculates a metric in between cells that is chosen by the user\n",
    "\n",
    "    Args:\n",
    "        input_file (str): path to input HDF5-format file\n",
    "        metric     (str): metric for calculation (mutual info, euclidean dist, pearson or spearman correlations\n",
    "    \"\"\"\n",
    "\n",
    "    #mat = pd.HDFStore(input_file)\n",
    "    #metrics = metric.lower()\n",
    "    #params = mat[\"params\"]\n",
    "    #mat1 = mat[params.loc[\"key1\", 0]]\n",
    "    #mat2 = mat[params.loc[\"key2\", 0]]\n",
    "    #mat.close()\n",
    "    \n",
    "    #ceb set up an MPI rank list and coordinator to read in appropriate matrix slice files, \n",
    "    # compute distance matrix blocks and store results in distributed distance matrix\n",
    "\n",
    "    #tmp hardcode input file\n",
    "    inputfileA = '../../test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad'\n",
    "    inputfileB = '../../test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad'\n",
    "\n",
    "    #For given rank id, read in A and B matrix slices \n",
    "    matA = anndata.read_h5ad(inputfileA)\n",
    "    matB = anndata.read_h5ad(inputfileB)\n",
    "    metrics = metric.lower()\n",
    "    params = matA[\"params\"]\n",
    "\n",
    "    #utils.calc_distance_mat(matA, matB, params, method=metrics)\n",
    "    calc_distance_mat_distributed(matA, matB, params, method=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = '../../test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad'\n",
    "calc_scatter(inputfile,'mi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#%memit\n",
    "start = time.time()\n",
    "embedding = PCA(n_components=100)\n",
    "frame_dr = embedding.fit_transform(frame)\n",
    "frame_dr.shape\n",
    "end = time.time()\n",
    "runtime = end - start\n",
    "msg = \"The runtime for PCA took {} seconds to complete\".format(runtime)\n",
    "logging.info(msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_mi_f(arr1, arr2, bins, m):\n",
    "    \"\"\" Calculates mutual information in between two cells, considering their gene expression levels\n",
    "\n",
    "    This function is called by calc_distance_mat. It takes gene expression data from single cells,\n",
    "    and compares them using standard calculation for mutual information. It builds a 2d histogram,\n",
    "    which is used to calculate P(arr1, arr2)\n",
    "\n",
    "    Args:\n",
    "        arr1 (pandas series): gene expression data for a given cell in matrix_1\n",
    "        arr2 (pandas series):\n",
    "        bins           (int):\n",
    "        m              (int):\n",
    "    \"\"\"\n",
    "    fq = fast_histogram.histogram2d(arr1, arr2, range=[[arr1.min(), arr1.max()+1e-9], [arr2.min(), arr2.max()+1e-9]],\n",
    "                                    bins=(bins, bins)) / float(m)\n",
    "    sm = np.sum(fq * float(m), axis=1)\n",
    "    tm = np.sum(fq * float(m), axis=0)\n",
    "    sm = np.asmatrix(sm / float(sm.sum()))\n",
    "    tm = np.asmatrix(tm / float(tm.sum()))\n",
    "    sm_tm = np.matmul(np.transpose(sm), tm)\n",
    "    div = np.divide(fq, sm_tm, where=sm_tm != 0, out=np.zeros_like(fq))\n",
    "    ent = np.log(div, where=div != 0, out=np.zeros_like(div))\n",
    "    agg = np.multiply(fq, ent, out=np.zeros_like(fq), where=fq != 0)\n",
    "    return agg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_bins = int((frame_dr.shape[0]) ** (1 / 3.0))\n",
    "num_genes = frame_dr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%timeit calc_mi_f(frame_dr[0], frame_dr[1], num_bins, num_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "arr = frame_dr[0]\n",
    "fast_histogram.histogram1d(arr, bins=num_bins, range=[arr.min(), arr.max()+1e-9]) / num_genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_cells = frame_dr.shape[0]\n",
    "marginals = np.empty((num_cells, num_bins))\n",
    "for index, cell in enumerate(frame_dr):\n",
    "    ht1d = fast_histogram.histogram1d(cell, bins=num_bins, range=[cell.min(), cell.max()+1e-9]) / num_genes\n",
    "    marginals[index] = ht1d\n",
    "print(marginals[0])\n",
    "print(marginals[1])\n",
    "np.transpose(np.asmatrix(marginals[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_mi_f2(arr1, arr2, marginals, index1, index2, bins, m):\n",
    "    \"\"\" Calculates mutual information in between two cells, considering their gene expression levels\n",
    "\n",
    "    This function is called by calc_distance_mat. It takes gene expression data from single cells,\n",
    "    and compares them using standard calculation for mutual information. It builds a 2d histogram,\n",
    "    which is used to calculate P(arr1, arr2)\n",
    "\n",
    "    Args:\n",
    "        arr1 (pandas series): gene expression data for a given cell in matrix_1\n",
    "        arr2 (pandas series):\n",
    "        bins           (int):\n",
    "        m              (int):\n",
    "    \"\"\"\n",
    "    fq = fast_histogram.histogram2d(arr1, arr2, range=[[arr1.min(), arr1.max()+1e-9], [arr2.min(), arr2.max()+1e-9]],\n",
    "                                    bins=(bins, bins)) / float(m)\n",
    "    sm_tm = np.matmul(np.transpose(np.asmatrix(marginals[index1])), np.asmatrix(marginals[index2]))\n",
    "    div = np.divide(fq, sm_tm, where=sm_tm != 0, out=np.zeros_like(fq))\n",
    "    ent = np.log(div, where=div != 0, out=np.zeros_like(div))\n",
    "    agg = np.multiply(fq, ent, out=np.zeros_like(fq), where=fq != 0)\n",
    "    return agg.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#%timeit\n",
    "calc_mi_f2(frame_dr[0], frame_dr[1], marginals, 0, 1, num_bins, num_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_marginals(frame_dr, num_bins, num_genes):\n",
    "    num_cells = frame_dr.shape[0]\n",
    "    marginals = np.empty((num_cells, num_bins))\n",
    "    for index, cell in enumerate(frame_dr):\n",
    "        ht1d = fast_histogram.histogram1d(cell, bins=num_bins, range=[cell.min(), cell.max() + 1e-9]) / num_genes\n",
    "        marginals[index] = ht1d\n",
    "    np.transpose(np.asmatrix(marginals[0]))\n",
    "    return marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_norm_mi_marginal(arr1, arr2, marginals, index1, index2, bins, m):\n",
    "    \"\"\" Calculates a normalized mutual information distance in between two cells\n",
    "\n",
    "    It takes gene expression data from single cells, and compares them using standard calculation for\n",
    "    mutual information. It builds a 2d histogram, which is used to calculate P(arr1, arr2)\n",
    "\n",
    "    Args:\n",
    "        arr1 (pandas series): gene expression data for a given cell in matrix_1\n",
    "        arr2 (pandas series):\n",
    "        bins           (int):\n",
    "        m              (int):\n",
    "    \"\"\"\n",
    "    fq = fast_histogram.histogram2d(arr1, arr2, range=[[arr1.min(), arr1.max()+1e-9], [arr2.min(), arr2.max()+1e-9]],\n",
    "                                    bins=(bins, bins)) / float(m)\n",
    "    sm_tm = np.matmul(np.transpose(np.asmatrix(marginals[index1])), np.asmatrix(marginals[index2]))\n",
    "    div = np.divide(fq, sm_tm, where=sm_tm != 0, out=np.zeros_like(fq))\n",
    "    ent = np.log(div, where=div != 0, out=np.zeros_like(div))\n",
    "    agg = np.multiply(fq, ent, out=np.zeros_like(fq), where=fq != 0)\n",
    "    joint_ent = -np.multiply(fq, np.log(fq, where=fq != 0, out=np.zeros_like(fq)),\n",
    "                             out=np.zeros_like(fq), where=fq != 0).sum()\n",
    "    return (joint_ent - agg.sum()) / joint_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %timeit\n",
    "calc_norm_mi_marginal(frame_dr[0], frame_dr[1], marginals, 0, 1, num_bins, num_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calc_norm_mi_marginal(frame_dr[0], frame_dr[2], marginals, 0, 2, num_bins, num_genes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_norm_mi(arr1, arr2, bins, m):\n",
    "    \"\"\" Calculates a normalized mutual information distance in between two cells\n",
    "\n",
    "    It takes gene expression data from single cells, and compares them using standard calculation for\n",
    "    mutual information. It builds a 2d histogram, which is used to calculate P(arr1, arr2)\n",
    "\n",
    "    Args:\n",
    "        arr1 (pandas series): gene expression data for a given cell in matrix_1\n",
    "        arr2 (pandas series):\n",
    "        bins           (int):\n",
    "        m              (int):\n",
    "    \"\"\"\n",
    "    fq = fast_histogram.histogram2d(arr1, arr2, range=[[arr1.min(), arr1.max()+1e-9], [arr2.min(), arr2.max()+1e-9]],\n",
    "                                    bins=(bins, bins)) / float(m)\n",
    "    sm = np.sum(fq * float(m), axis=1)\n",
    "    tm = np.sum(fq * float(m), axis=0)\n",
    "    sm = np.asmatrix(sm / float(sm.sum()))\n",
    "    tm = np.asmatrix(tm / float(tm.sum()))\n",
    "    sm_tm = np.matmul(np.transpose(sm), tm)\n",
    "    div = np.divide(fq, sm_tm, where=sm_tm != 0, out=np.zeros_like(fq))\n",
    "    ent = np.log(div, where=div != 0, out=np.zeros_like(div))\n",
    "    agg = np.multiply(fq, ent, out=np.zeros_like(fq), where=fq != 0)\n",
    "    joint_ent = -np.multiply(fq, np.log(fq, where=fq != 0, out=np.zeros_like(fq)),\n",
    "                             out=np.zeros_like(fq), where=fq != 0).sum()\n",
    "    return (joint_ent - agg.sum()) / joint_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%timeit calc_norm_mi(frame_dr[0], frame_dr[1], num_bins, num_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric\n",
    "dist = DistanceMetric.get_metric('euclidean')\n",
    "X = [frame_dr[0], frame_dr[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%timeit dist.pairwise(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_preprocessed_mat(in_file):\n",
    "    \"\"\"Read in preprocessed matrix file into a dataframe.\"\"\"\n",
    "    if in_file.endswith('.txt'):\n",
    "        frame = pd.read_csv(in_file, sep=\"\\t\", index_col=0).iloc[:, 0:]\n",
    "    if in_file.endswith('.h5ad') or in_file.endswith('.h5'):\n",
    "        adata = anndata.read_h5ad(in_file)\n",
    "        frame = adata.to_df()\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adata = anndata.read_h5ad('/Users/lding/Git/MICA/test_data/inputs/10x/PBMC/3k/pre-processed/pbmc3k_preprocessed.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(adata.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "indices, dists, forest = sc.neighbors.compute_neighbors_umap(adata.X, n_neighbors=10)\n",
    "end = time.time()\n",
    "runtime = end - start\n",
    "msg = \"The runtime for compute_neighbors_umap took {} seconds to complete\".format(runtime)\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_norm_mi(arr1, arr2, bins, m):\n",
    "    \"\"\" Calculates a normalized mutual information distance D(X, Y) = 1 - I(X, Y)/H(X, Y) using bin-based method\n",
    "\n",
    "    It takes gene expression data from single cells, and compares them using standard calculation for\n",
    "    mutual information and joint entropy. It builds a 2d histogram, which is used to calculate P(arr1, arr2).\n",
    "\n",
    "    Args:\n",
    "        arr1 (pandas series): gene expression data for cell 1\n",
    "        arr2 (pandas series): gene expression data for cell 2\n",
    "        marginals  (ndarray): marginal probability matrix\n",
    "        index1         (int): index of cell 1\n",
    "        index2         (int): index of cell 2\n",
    "        bins           (int): number of bins\n",
    "        m              (int): number of genes\n",
    "    Returns:\n",
    "        a float between 0 and 1\n",
    "    \"\"\"\n",
    "    fq = fast_histogram.histogram2d(arr1, arr2, range=[[arr1.min(), arr1.max()+1e-9], [arr2.min(), arr2.max()+1e-9]],\n",
    "                                    bins=(bins, bins)) / float(m)\n",
    "    sm = np.sum(fq * float(m), axis=1)\n",
    "    tm = np.sum(fq * float(m), axis=0)\n",
    "    sm = np.asmatrix(sm / float(sm.sum()))\n",
    "    tm = np.asmatrix(tm / float(tm.sum()))\n",
    "    sm_tm = np.matmul(np.transpose(sm), tm)\n",
    "    div = np.divide(fq, sm_tm, where=sm_tm != 0, out=np.zeros_like(fq))\n",
    "    ent = np.log(div, where=div != 0, out=np.zeros_like(div))\n",
    "    agg = np.multiply(fq, ent, out=np.zeros_like(fq), where=fq != 0)\n",
    "    joint_ent = -np.multiply(fq, np.log(fq, where=fq != 0, out=np.zeros_like(fq)),\n",
    "                             out=np.zeros_like(fq), where=fq != 0).sum()\n",
    "    return (joint_ent - agg.sum()) / joint_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_bins = int((adata.X.shape[0]) ** (1 / 3.0))\n",
    "num_genes = adata.X.shape[1]\n",
    "metric_params = {\"bins\": num_bins, \"m\": num_genes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Failed due to Numba\n"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "indices, dists, forest = sc.neighbors.compute_neighbors_umap(adata.X, n_neighbors=10, metric=calc_norm_mi,\n",
    "                                                             metric_kwds=metric_params)\n",
    "end = time.time()\n",
    "runtime = end - start\n",
    "msg = \"The runtime for compute_neighbors_umap took {} seconds to complete\".format(runtime)\n",
    "logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Test on 33k PBMC\n"
    }
   },
   "outputs": [],
   "source": [
    "adata = anndata.read_h5ad('/Users/lding/Documents/MICA/Datasets/filtered_gene_bc_matrices/hg19/pbmc33k_preprocessed.h5ad')\n",
    "frame = adata.to_df()\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "frame.to_csv('/Users/lding/Documents/MICA/kgraph/pmbc33k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.arange(1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
